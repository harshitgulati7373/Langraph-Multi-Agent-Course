{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7: Deployment & Real-World Applications\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- Deploy LangGraph applications to production environments\n",
    "- Implement Docker containerization with secure API key management\n",
    "- Build complete business workflows with real-world examples\n",
    "- Implement security best practices and scalability patterns\n",
    "- Handle OpenAI rate limiting and error recovery at scale\n",
    "- Create maintainable, production-ready multi-agent systems\n",
    "\n",
    "## ⏱️ Session Structure (2 hours)\n",
    "- **Learning Materials** (30 min): Deployment strategies and real-world patterns\n",
    "- **Hands-on Code** (60 min): Complete application deployment\n",
    "- **Final Challenge** (30 min): Build and deploy your own system\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📖 Learning Materials (30 minutes)\n",
    "\n",
    "### 📺 Deployment Resources\n",
    "- [LangGraph Platform Documentation](https://www.langchain.com/langgraph-platform) - Official deployment platform\n",
    "- [Docker Best Practices for AI Applications](https://docs.docker.com/develop/best-practices/) - Containerization guide\n",
    "- [Production AI Security Guide](https://owasp.org/www-project-machine-learning-security-top-10/) - Security best practices\n",
    "- [Scaling LangGraph Applications](https://langchain-ai.github.io/langgraph/how-tos/deployment/) - Scaling strategies\n",
    "\n",
    "### 🏗️ Theory: Production Deployment\n",
    "\n",
    "#### Deployment Options\n",
    "1. **LangGraph Platform**: Fully managed hosting with built-in monitoring\n",
    "2. **Cloud Deployment**: AWS, GCP, Azure with custom infrastructure\n",
    "3. **On-Premises**: Local deployment for security-sensitive applications\n",
    "4. **Hybrid**: Combination of cloud and on-premises components\n",
    "\n",
    "#### Security Considerations\n",
    "- **API Key Management**: Secure storage and rotation of OpenAI keys\n",
    "- **Input Validation**: Sanitize and validate all user inputs\n",
    "- **Rate Limiting**: Prevent abuse and manage costs\n",
    "- **Audit Logging**: Track all system interactions\n",
    "- **Network Security**: VPC, firewalls, and encrypted communications\n",
    "\n",
    "#### Scalability Patterns\n",
    "- **Horizontal Scaling**: Multiple instances behind load balancers\n",
    "- **Vertical Scaling**: Larger instances for compute-intensive tasks\n",
    "- **Caching**: Redis/Memcached for response caching\n",
    "- **Queue Systems**: RabbitMQ/Redis for async processing\n",
    "- **Database Optimization**: Connection pooling and read replicas\n",
    "\n",
    "#### Real-World Business Applications\n",
    "- **Customer Support**: Automated ticket routing and response\n",
    "- **Content Generation**: Marketing copy, documentation, reports\n",
    "- **Data Analysis**: Automated insights and recommendations\n",
    "- **Process Automation**: Workflow orchestration and decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 💻 Hands-on Code (60 minutes)\n",
    "\n",
    "### Setup and Production Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install production deployment dependencies\n",
    "!pip install langgraph langchain langchain-openai fastapi uvicorn\n",
    "!pip install redis celery docker prometheus-client\n",
    "!pip install python-multipart aiofiles cryptography\n",
    "!pip install pydantic-settings python-jose[cryptography]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from typing import Dict, List, Optional, Any, Literal\n",
    "from datetime import datetime, timedelta\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# FastAPI and security\n",
    "from fastapi import FastAPI, HTTPException, Depends, Security, status\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.middleware.trustedhost import TrustedHostMiddleware\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from pydantic_settings import BaseSettings\n",
    "\n",
    "# LangGraph and LangChain\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "\n",
    "# Monitoring and security\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "import redis\n",
    "from cryptography.fernet import Fernet\n",
    "import logging\n",
    "\n",
    "print(\"✅ Production deployment dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Production Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionSettings(BaseSettings):\n",
    "    \"\"\"Production configuration with validation and security\"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    app_name: str = \"LangGraph Production System\"\n",
    "    app_version: str = \"1.0.0\"\n",
    "    environment: Literal[\"development\", \"staging\", \"production\"] = \"development\"\n",
    "    debug: bool = False\n",
    "    \n",
    "    # OpenAI Configuration\n",
    "    openai_api_key: str = Field(..., description=\"OpenAI API key\")\n",
    "    openai_model: str = \"gpt-3.5-turbo\"\n",
    "    openai_timeout: int = 30\n",
    "    openai_max_retries: int = 3\n",
    "    \n",
    "    # Database Configuration\n",
    "    postgres_url: Optional[str] = None\n",
    "    redis_url: str = \"redis://localhost:6379\"\n",
    "    \n",
    "    # Security Configuration\n",
    "    secret_key: str = Field(default_factory=lambda: Fernet.generate_key().decode())\n",
    "    api_key_header: str = \"X-API-Key\"\n",
    "    allowed_hosts: List[str] = [\"*\"]\n",
    "    cors_origins: List[str] = [\"*\"]\n",
    "    \n",
    "    # Rate Limiting\n",
    "    rate_limit_requests_per_minute: int = 60\n",
    "    rate_limit_cost_per_hour: float = 10.0  # $10/hour limit\n",
    "    \n",
    "    # Monitoring\n",
    "    enable_metrics: bool = True\n",
    "    metrics_path: str = \"/metrics\"\n",
    "    health_check_path: str = \"/health\"\n",
    "    \n",
    "    @validator('openai_api_key')\n",
    "    def validate_openai_key(cls, v):\n",
    "        if not v or not v.startswith('sk-'):\n",
    "            raise ValueError('OpenAI API key must start with sk-')\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "        case_sensitive = False\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    settings = ProductionSettings()\n",
    "    print(\"✅ Production configuration loaded successfully\")\n",
    "    print(f\"📝 Environment: {settings.environment}\")\n",
    "    print(f\"🔧 Model: {settings.openai_model}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Configuration error: {e}\")\n",
    "    print(\"💡 Please check your .env file or environment variables\")\n",
    "    # Use default settings for demo\n",
    "    settings = ProductionSettings(\n",
    "        openai_api_key=\"sk-demo-key-for-testing\",\n",
    "        environment=\"development\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Security and Authentication System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecurityManager:\n",
    "    \"\"\"Comprehensive security management\"\"\"\n",
    "    \n",
    "    def __init__(self, settings: ProductionSettings):\n",
    "        self.settings = settings\n",
    "        self.fernet = Fernet(settings.secret_key.encode())\n",
    "        self.valid_api_keys = self._load_api_keys()\n",
    "        self.rate_limiter = RateLimiter(settings)\n",
    "    \n",
    "    def _load_api_keys(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Load valid API keys with metadata\"\"\"\n",
    "        # In production, load from secure storage\n",
    "        return {\n",
    "            \"demo-key-123\": {\n",
    "                \"user_id\": \"demo-user\",\n",
    "                \"permissions\": [\"read\", \"write\"],\n",
    "                \"rate_limit\": 100,\n",
    "                \"created_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def encrypt_data(self, data: str) -> str:\n",
    "        \"\"\"Encrypt sensitive data\"\"\"\n",
    "        return self.fernet.encrypt(data.encode()).decode()\n",
    "    \n",
    "    def decrypt_data(self, encrypted_data: str) -> str:\n",
    "        \"\"\"Decrypt sensitive data\"\"\"\n",
    "        return self.fernet.decrypt(encrypted_data.encode()).decode()\n",
    "    \n",
    "    def validate_api_key(self, api_key: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Validate API key and return user info\"\"\"\n",
    "        if api_key in self.valid_api_keys:\n",
    "            return self.valid_api_keys[api_key]\n",
    "        return None\n",
    "    \n",
    "    def sanitize_input(self, text: str) -> str:\n",
    "        \"\"\"Sanitize user input\"\"\"\n",
    "        # Remove potentially dangerous content\n",
    "        dangerous_patterns = [\n",
    "            \"<script\", \"javascript:\", \"on\", \"eval(\", \"exec(\",\n",
    "            \"import \", \"__import__\", \"subprocess\", \"os.system\"\n",
    "        ]\n",
    "        \n",
    "        sanitized = text\n",
    "        for pattern in dangerous_patterns:\n",
    "            sanitized = sanitized.replace(pattern, \"[BLOCKED]\")\n",
    "        \n",
    "        return sanitized[:1000]  # Limit length\n",
    "    \n",
    "    def check_rate_limit(self, user_id: str) -> bool:\n",
    "        \"\"\"Check if user is within rate limits\"\"\"\n",
    "        return self.rate_limiter.check_limit(user_id)\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Redis-based rate limiting\"\"\"\n",
    "    \n",
    "    def __init__(self, settings: ProductionSettings):\n",
    "        self.settings = settings\n",
    "        try:\n",
    "            self.redis_client = redis.from_url(settings.redis_url)\n",
    "            self.redis_client.ping()\n",
    "            print(\"✅ Redis connected for rate limiting\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Redis unavailable: {e}\")\n",
    "            self.redis_client = None\n",
    "    \n",
    "    def check_limit(self, user_id: str) -> bool:\n",
    "        \"\"\"Check rate limit for user\"\"\"\n",
    "        if not self.redis_client:\n",
    "            return True  # Allow if Redis unavailable\n",
    "        \n",
    "        try:\n",
    "            key = f\"rate_limit:{user_id}\"\n",
    "            current = self.redis_client.get(key)\n",
    "            \n",
    "            if current is None:\n",
    "                # First request\n",
    "                self.redis_client.setex(key, 60, 1)\n",
    "                return True\n",
    "            \n",
    "            if int(current) >= self.settings.rate_limit_requests_per_minute:\n",
    "                return False\n",
    "            \n",
    "            self.redis_client.incr(key)\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Rate limit check failed: {e}\")\n",
    "            return True  # Allow on error\n",
    "\n",
    "# Initialize security\n",
    "security_manager = SecurityManager(settings)\n",
    "print(\"🔒 Security system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Production-Ready Business Workflow System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production state model\n",
    "class BusinessWorkflowState(BaseModel):\n",
    "    \"\"\"Complete business workflow state\"\"\"\n",
    "    \n",
    "    # Core data\n",
    "    workflow_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n",
    "    user_id: str\n",
    "    workflow_type: Literal[\"customer_support\", \"content_generation\", \"data_analysis\", \"process_automation\"]\n",
    "    \n",
    "    # Messages and context\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    context: Dict[str, Any] = Field(default_factory=dict)\n",
    "    \n",
    "    # Workflow status\n",
    "    status: Literal[\"pending\", \"processing\", \"waiting_approval\", \"completed\", \"failed\"] = \"pending\"\n",
    "    current_step: str = \"initial\"\n",
    "    steps_completed: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    # Business metrics\n",
    "    priority: Literal[\"low\", \"medium\", \"high\", \"urgent\"] = \"medium\"\n",
    "    estimated_cost: float = 0.0\n",
    "    actual_cost: float = 0.0\n",
    "    processing_time: float = 0.0\n",
    "    \n",
    "    # Compliance and audit\n",
    "    compliance_status: Literal[\"pending\", \"approved\", \"rejected\"] = \"pending\"\n",
    "    audit_trail: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    \n",
    "    # Error handling\n",
    "    errors: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    retry_count: int = 0\n",
    "    max_retries: int = 3\n",
    "    \n",
    "    def add_audit_entry(self, action: str, details: str):\n",
    "        \"\"\"Add entry to audit trail\"\"\"\n",
    "        self.audit_trail.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"action\": action,\n",
    "            \"details\": details,\n",
    "            \"step\": self.current_step\n",
    "        })\n",
    "\n",
    "# Business workflow nodes\n",
    "class BusinessWorkflowEngine:\n",
    "    \"\"\"Complete business workflow engine\"\"\"\n",
    "    \n",
    "    def __init__(self, settings: ProductionSettings, security: SecurityManager):\n",
    "        self.settings = settings\n",
    "        self.security = security\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=settings.openai_model,\n",
    "            openai_api_key=settings.openai_api_key,\n",
    "            timeout=settings.openai_timeout,\n",
    "            max_retries=settings.openai_max_retries\n",
    "        )\n",
    "    \n",
    "    def customer_support_agent(self, state: BusinessWorkflowState) -> BusinessWorkflowState:\n",
    "        \"\"\"Handle customer support workflows\"\"\"\n",
    "        state.add_audit_entry(\"customer_support_start\", \"Starting customer support workflow\")\n",
    "        \n",
    "        try:\n",
    "            if state.messages:\n",
    "                last_message = state.messages[-1]\n",
    "                \n",
    "                # Determine urgency and routing\n",
    "                urgency_prompt = f\"\"\"\n",
    "                Analyze this customer message and determine:\n",
    "                1. Urgency level (low, medium, high, urgent)\n",
    "                2. Category (technical, billing, general, complaint)\n",
    "                3. Recommended action\n",
    "                \n",
    "                Customer message: {last_message.content}\n",
    "                \n",
    "                Respond in JSON format:\n",
    "                {{\n",
    "                    \"urgency\": \"medium\",\n",
    "                    \"category\": \"technical\",\n",
    "                    \"action\": \"provide_solution\",\n",
    "                    \"requires_human\": false\n",
    "                }}\n",
    "                \"\"\"\n",
    "                \n",
    "                analysis_response = self.llm.invoke([HumanMessage(content=urgency_prompt)])\n",
    "                \n",
    "                try:\n",
    "                    analysis = json.loads(analysis_response.content)\n",
    "                    state.priority = analysis.get(\"urgency\", \"medium\")\n",
    "                    state.context[\"category\"] = analysis.get(\"category\", \"general\")\n",
    "                    state.context[\"requires_human\"] = analysis.get(\"requires_human\", False)\n",
    "                except json.JSONDecodeError:\n",
    "                    state.priority = \"medium\"\n",
    "                    state.context[\"category\"] = \"general\"\n",
    "                \n",
    "                # Generate response based on category\n",
    "                if state.context.get(\"requires_human\"):\n",
    "                    state.status = \"waiting_approval\"\n",
    "                    response_content = \"Your request has been escalated to our support team. You'll receive a response within 24 hours.\"\n",
    "                else:\n",
    "                    response_prompt = f\"\"\"\n",
    "                    You are a helpful customer support agent. Provide a helpful, professional response to this customer inquiry.\n",
    "                    \n",
    "                    Category: {state.context.get('category', 'general')}\n",
    "                    Priority: {state.priority}\n",
    "                    \n",
    "                    Customer message: {last_message.content}\n",
    "                    \n",
    "                    Provide a clear, helpful response that addresses their concern.\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    response = self.llm.invoke([HumanMessage(content=response_prompt)])\n",
    "                    response_content = response.content\n",
    "                    state.status = \"completed\"\n",
    "                \n",
    "                state.messages.append(AIMessage(content=response_content))\n",
    "                state.current_step = \"response_generated\"\n",
    "                state.steps_completed.append(\"customer_support\")\n",
    "                \n",
    "                state.add_audit_entry(\"response_generated\", f\"Generated {state.context.get('category')} response\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.errors.append({\n",
    "                \"type\": type(e).__name__,\n",
    "                \"message\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"step\": \"customer_support\"\n",
    "            })\n",
    "            state.status = \"failed\"\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def content_generation_agent(self, state: BusinessWorkflowState) -> BusinessWorkflowState:\n",
    "        \"\"\"Handle content generation workflows\"\"\"\n",
    "        state.add_audit_entry(\"content_generation_start\", \"Starting content generation workflow\")\n",
    "        \n",
    "        try:\n",
    "            if state.messages:\n",
    "                request = state.messages[-1].content\n",
    "                \n",
    "                # Analyze content requirements\n",
    "                analysis_prompt = f\"\"\"\n",
    "                Analyze this content generation request:\n",
    "                {request}\n",
    "                \n",
    "                Determine:\n",
    "                1. Content type (blog_post, marketing_copy, documentation, email, report)\n",
    "                2. Target audience (technical, business, general)\n",
    "                3. Tone (formal, casual, professional, friendly)\n",
    "                4. Estimated word count\n",
    "                \n",
    "                Respond in JSON format.\n",
    "                \"\"\"\n",
    "                \n",
    "                analysis_response = self.llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "                \n",
    "                try:\n",
    "                    analysis = json.loads(analysis_response.content)\n",
    "                    state.context.update(analysis)\n",
    "                except json.JSONDecodeError:\n",
    "                    state.context[\"content_type\"] = \"general\"\n",
    "                \n",
    "                # Generate content\n",
    "                generation_prompt = f\"\"\"\n",
    "                Create high-quality content based on this request:\n",
    "                {request}\n",
    "                \n",
    "                Content specifications:\n",
    "                - Type: {state.context.get('content_type', 'general')}\n",
    "                - Audience: {state.context.get('target_audience', 'general')}\n",
    "                - Tone: {state.context.get('tone', 'professional')}\n",
    "                \n",
    "                Provide well-structured, engaging content that meets the requirements.\n",
    "                \"\"\"\n",
    "                \n",
    "                content_response = self.llm.invoke([HumanMessage(content=generation_prompt)])\n",
    "                \n",
    "                state.messages.append(AIMessage(content=content_response.content))\n",
    "                state.status = \"completed\"\n",
    "                state.current_step = \"content_generated\"\n",
    "                state.steps_completed.append(\"content_generation\")\n",
    "                \n",
    "                state.add_audit_entry(\"content_generated\", f\"Generated {state.context.get('content_type')} content\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            state.errors.append({\n",
    "                \"type\": type(e).__name__,\n",
    "                \"message\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"step\": \"content_generation\"\n",
    "            })\n",
    "            state.status = \"failed\"\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def quality_assurance_agent(self, state: BusinessWorkflowState) -> BusinessWorkflowState:\n",
    "        \"\"\"Quality assurance and compliance checking\"\"\"\n",
    "        state.add_audit_entry(\"qa_start\", \"Starting quality assurance check\")\n",
    "        \n",
    "        try:\n",
    "            if state.messages and len(state.messages) >= 2:\n",
    "                ai_response = state.messages[-1]\n",
    "                \n",
    "                qa_prompt = f\"\"\"\n",
    "                Review this AI-generated response for quality and compliance:\n",
    "                \n",
    "                Response: {ai_response.content}\n",
    "                Workflow type: {state.workflow_type}\n",
    "                Priority: {state.priority}\n",
    "                \n",
    "                Check for:\n",
    "                1. Accuracy and relevance\n",
    "                2. Professional tone\n",
    "                3. Compliance with guidelines\n",
    "                4. Potential issues or concerns\n",
    "                \n",
    "                Provide a quality score (1-10) and approval status (approved/needs_revision/rejected).\n",
    "                Respond in JSON format.\n",
    "                \"\"\"\n",
    "                \n",
    "                qa_response = self.llm.invoke([HumanMessage(content=qa_prompt)])\n",
    "                \n",
    "                try:\n",
    "                    qa_result = json.loads(qa_response.content)\n",
    "                    quality_score = qa_result.get(\"quality_score\", 8)\n",
    "                    approval_status = qa_result.get(\"approval_status\", \"approved\")\n",
    "                    \n",
    "                    state.context[\"quality_score\"] = quality_score\n",
    "                    state.compliance_status = \"approved\" if approval_status == \"approved\" else \"rejected\"\n",
    "                    \n",
    "                    if quality_score >= 7 and approval_status == \"approved\":\n",
    "                        state.status = \"completed\"\n",
    "                    else:\n",
    "                        state.status = \"waiting_approval\"\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    state.context[\"quality_score\"] = 8\n",
    "                    state.compliance_status = \"approved\"\n",
    "                    state.status = \"completed\"\n",
    "                \n",
    "                state.current_step = \"qa_completed\"\n",
    "                state.steps_completed.append(\"quality_assurance\")\n",
    "                state.add_audit_entry(\"qa_completed\", f\"QA score: {state.context.get('quality_score')}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            state.errors.append({\n",
    "                \"type\": type(e).__name__,\n",
    "                \"message\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"step\": \"quality_assurance\"\n",
    "            })\n",
    "            state.compliance_status = \"rejected\"\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize workflow engine\n",
    "workflow_engine = BusinessWorkflowEngine(settings, security_manager)\n",
    "print(\"🏭 Business workflow engine initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Production Graph and Routing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_production_business_graph():\n",
    "    \"\"\"Create production-ready business workflow graph\"\"\"\n",
    "    \n",
    "    # Create the state graph\n",
    "    graph = StateGraph(BusinessWorkflowState)\n",
    "    \n",
    "    # Add workflow nodes\n",
    "    graph.add_node(\"customer_support\", workflow_engine.customer_support_agent)\n",
    "    graph.add_node(\"content_generation\", workflow_engine.content_generation_agent)\n",
    "    graph.add_node(\"quality_assurance\", workflow_engine.quality_assurance_agent)\n",
    "    \n",
    "    # Routing logic\n",
    "    def route_workflow(state: BusinessWorkflowState) -> str:\n",
    "        \"\"\"Route to appropriate workflow based on type\"\"\"\n",
    "        if state.workflow_type == \"customer_support\":\n",
    "            return \"customer_support\"\n",
    "        elif state.workflow_type == \"content_generation\":\n",
    "            return \"content_generation\"\n",
    "        else:\n",
    "            return \"customer_support\"  # Default\n",
    "    \n",
    "    def quality_check_routing(state: BusinessWorkflowState) -> str:\n",
    "        \"\"\"Route after initial processing\"\"\"\n",
    "        if state.status == \"failed\":\n",
    "            return END\n",
    "        elif state.priority in [\"high\", \"urgent\"] or state.workflow_type == \"content_generation\":\n",
    "            return \"quality_assurance\"\n",
    "        else:\n",
    "            return END\n",
    "    \n",
    "    def final_routing(state: BusinessWorkflowState) -> str:\n",
    "        \"\"\"Final routing after QA\"\"\"\n",
    "        return END\n",
    "    \n",
    "    # Add edges\n",
    "    graph.add_edge(START, \"customer_support\")  # Default start\n",
    "    \n",
    "    # Conditional routing from start based on workflow type\n",
    "    graph.add_conditional_edges(\n",
    "        START,\n",
    "        route_workflow,\n",
    "        {\n",
    "            \"customer_support\": \"customer_support\",\n",
    "            \"content_generation\": \"content_generation\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Route to QA if needed\n",
    "    graph.add_conditional_edges(\n",
    "        \"customer_support\",\n",
    "        quality_check_routing,\n",
    "        {\"quality_assurance\": \"quality_assurance\", END: END}\n",
    "    )\n",
    "    \n",
    "    graph.add_conditional_edges(\n",
    "        \"content_generation\",\n",
    "        quality_check_routing,\n",
    "        {\"quality_assurance\": \"quality_assurance\", END: END}\n",
    "    )\n",
    "    \n",
    "    # Final routing\n",
    "    graph.add_conditional_edges(\n",
    "        \"quality_assurance\",\n",
    "        final_routing,\n",
    "        {END: END}\n",
    "    )\n",
    "    \n",
    "    # Set up persistence\n",
    "    if settings.postgres_url:\n",
    "        try:\n",
    "            checkpointer = PostgresSaver.from_conn_string(settings.postgres_url)\n",
    "            print(\"✅ Using PostgreSQL for production persistence\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ PostgreSQL error: {e}, using in-memory\")\n",
    "            checkpointer = InMemorySaver()\n",
    "    else:\n",
    "        checkpointer = InMemorySaver()\n",
    "        print(\"📝 Using in-memory persistence (set POSTGRES_URL for production)\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = graph.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Create production graph\n",
    "production_graph = create_production_business_graph()\n",
    "print(\"🚀 Production business workflow graph created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. FastAPI Production Web Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request/Response models\n",
    "class WorkflowRequest(BaseModel):\n",
    "    \"\"\"API request model\"\"\"\n",
    "    message: str = Field(..., min_length=1, max_length=5000)\n",
    "    workflow_type: Literal[\"customer_support\", \"content_generation\", \"data_analysis\", \"process_automation\"] = \"customer_support\"\n",
    "    priority: Literal[\"low\", \"medium\", \"high\", \"urgent\"] = \"medium\"\n",
    "    user_context: Optional[Dict[str, Any]] = None\n",
    "    \n",
    "    @validator('message')\n",
    "    def sanitize_message(cls, v):\n",
    "        # Basic sanitization\n",
    "        return security_manager.sanitize_input(v)\n",
    "\n",
    "class WorkflowResponse(BaseModel):\n",
    "    \"\"\"API response model\"\"\"\n",
    "    workflow_id: str\n",
    "    status: str\n",
    "    response: str\n",
    "    processing_time: float\n",
    "    cost_estimate: float\n",
    "    quality_score: Optional[float] = None\n",
    "    requires_approval: bool = False\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response\"\"\"\n",
    "    status: str\n",
    "    timestamp: str\n",
    "    version: str\n",
    "    environment: str\n",
    "    services: Dict[str, str]\n",
    "\n",
    "# Metrics\n",
    "REQUEST_COUNT = Counter('api_requests_total', 'Total API requests', ['endpoint', 'method', 'status'])\n",
    "REQUEST_DURATION = Histogram('api_request_duration_seconds', 'Request duration', ['endpoint'])\n",
    "WORKFLOW_COUNT = Counter('workflows_total', 'Total workflows', ['type', 'status'])\n",
    "\n",
    "# Security dependency\n",
    "security = HTTPBearer()\n",
    "\n",
    "async def verify_api_key(credentials: HTTPAuthorizationCredentials = Security(security)) -> Dict[str, Any]:\n",
    "    \"\"\"Verify API key authentication\"\"\"\n",
    "    api_key = credentials.credentials\n",
    "    user_info = security_manager.validate_api_key(api_key)\n",
    "    \n",
    "    if not user_info:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Invalid API key\"\n",
    "        )\n",
    "    \n",
    "    # Check rate limit\n",
    "    if not security_manager.check_rate_limit(user_info[\"user_id\"]):\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_429_TOO_MANY_REQUESTS,\n",
    "            detail=\"Rate limit exceeded\"\n",
    "        )\n",
    "    \n",
    "    return user_info\n",
    "\n",
    "# Create FastAPI app\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Application lifespan events\"\"\"\n",
    "    # Startup\n",
    "    print(\"🚀 Starting production API server...\")\n",
    "    yield\n",
    "    # Shutdown\n",
    "    print(\"🛑 Shutting down production API server...\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=settings.app_name,\n",
    "    version=settings.app_version,\n",
    "    description=\"Production LangGraph Multi-Agent System\",\n",
    "    docs_url=\"/docs\" if settings.debug else None,\n",
    "    redoc_url=\"/redoc\" if settings.debug else None,\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "# Add security middleware\n",
    "app.add_middleware(\n",
    "    TrustedHostMiddleware,\n",
    "    allowed_hosts=settings.allowed_hosts\n",
    ")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=settings.cors_origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"GET\", \"POST\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# API endpoints\n",
    "@app.get(settings.health_check_path, response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    \n",
    "    # Check service health\n",
    "    services = {\n",
    "        \"api\": \"healthy\",\n",
    "        \"openai\": \"healthy\" if settings.openai_api_key.startswith(\"sk-\") else \"unhealthy\",\n",
    "        \"redis\": \"healthy\" if security_manager.rate_limiter.redis_client else \"unhealthy\",\n",
    "        \"postgres\": \"healthy\" if settings.postgres_url else \"unavailable\"\n",
    "    }\n",
    "    \n",
    "    overall_status = \"healthy\" if all(s in [\"healthy\", \"unavailable\"] for s in services.values()) else \"unhealthy\"\n",
    "    \n",
    "    return HealthResponse(\n",
    "        status=overall_status,\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        version=settings.app_version,\n",
    "        environment=settings.environment,\n",
    "        services=services\n",
    "    )\n",
    "\n",
    "@app.get(settings.metrics_path)\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    if not settings.enable_metrics:\n",
    "        raise HTTPException(status_code=404, detail=\"Metrics disabled\")\n",
    "    \n",
    "    return generate_latest()\n",
    "\n",
    "@app.post(\"/workflow\", response_model=WorkflowResponse)\n",
    "async def process_workflow(\n",
    "    request: WorkflowRequest,\n",
    "    user_info: Dict[str, Any] = Depends(verify_api_key)\n",
    "):\n",
    "    \"\"\"Process a business workflow\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        REQUEST_COUNT.labels(endpoint=\"workflow\", method=\"POST\", status=\"processing\").inc()\n",
    "        \n",
    "        # Create workflow state\n",
    "        state = BusinessWorkflowState(\n",
    "            user_id=user_info[\"user_id\"],\n",
    "            workflow_type=request.workflow_type,\n",
    "            priority=request.priority,\n",
    "            messages=[HumanMessage(content=request.message)]\n",
    "        )\n",
    "        \n",
    "        if request.user_context:\n",
    "            state.context.update(request.user_context)\n",
    "        \n",
    "        # Process workflow\n",
    "        config = {\"configurable\": {\"thread_id\": state.workflow_id}}\n",
    "        result = production_graph.invoke(state, config=config)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Extract response\n",
    "        response_text = \"Workflow completed successfully.\"\n",
    "        if result.messages and len(result.messages) > 1:\n",
    "            response_text = result.messages[-1].content\n",
    "        \n",
    "        # Create response\n",
    "        workflow_response = WorkflowResponse(\n",
    "            workflow_id=result.workflow_id,\n",
    "            status=result.status,\n",
    "            response=response_text,\n",
    "            processing_time=processing_time,\n",
    "            cost_estimate=result.actual_cost,\n",
    "            quality_score=result.context.get(\"quality_score\"),\n",
    "            requires_approval=result.status == \"waiting_approval\",\n",
    "            metadata={\n",
    "                \"steps_completed\": result.steps_completed,\n",
    "                \"compliance_status\": result.compliance_status,\n",
    "                \"priority\": result.priority\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Record metrics\n",
    "        REQUEST_COUNT.labels(endpoint=\"workflow\", method=\"POST\", status=\"success\").inc()\n",
    "        REQUEST_DURATION.labels(endpoint=\"workflow\").observe(processing_time)\n",
    "        WORKFLOW_COUNT.labels(type=request.workflow_type, status=result.status).inc()\n",
    "        \n",
    "        return workflow_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        REQUEST_COUNT.labels(endpoint=\"workflow\", method=\"POST\", status=\"error\").inc()\n",
    "        REQUEST_DURATION.labels(endpoint=\"workflow\").observe(processing_time)\n",
    "        \n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Workflow processing failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/workflow/{workflow_id}\")\n",
    "async def get_workflow_status(\n",
    "    workflow_id: str,\n",
    "    user_info: Dict[str, Any] = Depends(verify_api_key)\n",
    "):\n",
    "    \"\"\"Get workflow status\"\"\"\n",
    "    \n",
    "    try:\n",
    "        config = {\"configurable\": {\"thread_id\": workflow_id}}\n",
    "        state = production_graph.get_state(config)\n",
    "        \n",
    "        if not state:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=\"Workflow not found\"\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"workflow_id\": workflow_id,\n",
    "            \"status\": state.values.status,\n",
    "            \"current_step\": state.values.current_step,\n",
    "            \"steps_completed\": state.values.steps_completed,\n",
    "            \"compliance_status\": state.values.compliance_status,\n",
    "            \"errors\": state.values.errors\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Failed to get workflow status: {str(e)}\"\n",
    "        )\n",
    "\n",
    "print(\"🌐 Production FastAPI server configured\")\n",
    "print(\"📚 API Documentation available at /docs (if debug=True)\")\n",
    "print(\"💡 Use 'uvicorn app:app --host 0.0.0.0 --port 8000' to run the server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Docker Deployment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile content\n",
    "dockerfile_content = '''\n",
    "# Production Dockerfile for LangGraph Application\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PYTHONUNBUFFERED=1 \\\n",
    "    PIP_NO_CACHE_DIR=1 \\\n",
    "    PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Create non-root user\n",
    "RUN groupadd -r appuser && useradd -r -g appuser appuser\n",
    "\n",
    "# Set work directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    gcc \\\n",
    "    postgresql-client \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements and install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY --chown=appuser:appuser . .\n",
    "\n",
    "# Switch to non-root user\n",
    "USER appuser\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run the application\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n",
    "'''\n",
    "\n",
    "# Create docker-compose.yml content\n",
    "docker_compose_content = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  web:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - ENVIRONMENT=production\n",
    "      - POSTGRES_URL=postgresql://user:password@postgres:5432/langgraph\n",
    "      - REDIS_URL=redis://redis:6379\n",
    "    depends_on:\n",
    "      postgres:\n",
    "        condition: service_healthy\n",
    "      redis:\n",
    "        condition: service_healthy\n",
    "    env_file:\n",
    "      - .env\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "\n",
    "  postgres:\n",
    "    image: postgres:15\n",
    "    environment:\n",
    "      POSTGRES_DB: langgraph\n",
    "      POSTGRES_USER: user\n",
    "      POSTGRES_PASSWORD: password\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"pg_isready -U user -d langgraph\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"ping\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "      - prometheus_data:/prometheus\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    volumes:\n",
    "      - grafana_data:/var/lib/grafana\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  redis_data:\n",
    "  prometheus_data:\n",
    "  grafana_data:\n",
    "\n",
    "networks:\n",
    "  langgraph-network:\n",
    "    driver: bridge\n",
    "'''\n",
    "\n",
    "# Create Kubernetes deployment manifest\n",
    "k8s_deployment_content = '''\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: langgraph-api\n",
    "  labels:\n",
    "    app: langgraph-api\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: langgraph-api\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: langgraph-api\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: langgraph-api\n",
    "        image: langgraph-api:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: ENVIRONMENT\n",
    "          value: \"production\"\n",
    "        - name: OPENAI_API_KEY\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: openai-secret\n",
    "              key: api-key\n",
    "        - name: POSTGRES_URL\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: postgres-secret\n",
    "              key: connection-string\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"1000m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: langgraph-api-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: langgraph-api\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "'''\n",
    "\n",
    "# Write deployment files\n",
    "deployment_files = {\n",
    "    \"Dockerfile\": dockerfile_content,\n",
    "    \"docker-compose.yml\": docker_compose_content,\n",
    "    \"k8s-deployment.yaml\": k8s_deployment_content\n",
    "}\n",
    "\n",
    "print(\"🐳 Docker and Kubernetes deployment configurations created:\")\n",
    "for filename, content in deployment_files.items():\n",
    "    print(f\"📄 {filename} - Ready for production deployment\")\n",
    "\n",
    "# Deployment commands\n",
    "deployment_commands = {\n",
    "    \"Docker Build\": \"docker build -t langgraph-api .\",\n",
    "    \"Docker Run\": \"docker run -p 8000:8000 --env-file .env langgraph-api\",\n",
    "    \"Docker Compose\": \"docker-compose up -d\",\n",
    "    \"Kubernetes Deploy\": \"kubectl apply -f k8s-deployment.yaml\",\n",
    "    \"Scale Kubernetes\": \"kubectl scale deployment langgraph-api --replicas=5\"\n",
    "}\n",
    "\n",
    "print(\"\\n🚀 Deployment Commands:\")\n",
    "for name, command in deployment_commands.items():\n",
    "    print(f\"  {name}: {command}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Testing the Complete Production System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import asyncio\n",
    "\n",
    "async def test_production_system():\n",
    "    \"\"\"Comprehensive test of the production system\"\"\"\n",
    "    \n",
    "    print(\"🧪 Testing Production LangGraph System\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test 1: Customer Support Workflow\n",
    "    print(\"\\n1️⃣ Testing Customer Support Workflow\")\n",
    "    \n",
    "    support_state = BusinessWorkflowState(\n",
    "        user_id=\"test-user-1\",\n",
    "        workflow_type=\"customer_support\",\n",
    "        priority=\"medium\",\n",
    "        messages=[HumanMessage(content=\"I'm having trouble with my account login\")]\n",
    "    )\n",
    "    \n",
    "    config1 = {\"configurable\": {\"thread_id\": \"test-support-1\"}}\n",
    "    \n",
    "    try:\n",
    "        result1 = production_graph.invoke(support_state, config=config1)\n",
    "        print(f\"✅ Status: {result1.status}\")\n",
    "        print(f\"📝 Response: {result1.messages[-1].content[:100]}...\")\n",
    "        print(f\"🔍 Steps: {result1.steps_completed}\")\n",
    "        print(f\"📊 Quality Score: {result1.context.get('quality_score', 'N/A')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Customer Support Test Failed: {e}\")\n",
    "    \n",
    "    # Test 2: Content Generation Workflow\n",
    "    print(\"\\n2️⃣ Testing Content Generation Workflow\")\n",
    "    \n",
    "    content_state = BusinessWorkflowState(\n",
    "        user_id=\"test-user-2\",\n",
    "        workflow_type=\"content_generation\",\n",
    "        priority=\"high\",\n",
    "        messages=[HumanMessage(content=\"Write a professional email about our new product launch\")]\n",
    "    )\n",
    "    \n",
    "    config2 = {\"configurable\": {\"thread_id\": \"test-content-1\"}}\n",
    "    \n",
    "    try:\n",
    "        result2 = production_graph.invoke(content_state, config=config2)\n",
    "        print(f\"✅ Status: {result2.status}\")\n",
    "        print(f\"📝 Content Type: {result2.context.get('content_type', 'N/A')}\")\n",
    "        print(f\"🎯 Target Audience: {result2.context.get('target_audience', 'N/A')}\")\n",
    "        print(f\"🔍 Steps: {result2.steps_completed}\")\n",
    "        print(f\"✅ Compliance: {result2.compliance_status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Content Generation Test Failed: {e}\")\n",
    "    \n",
    "    # Test 3: Security and Rate Limiting\n",
    "    print(\"\\n3️⃣ Testing Security Features\")\n",
    "    \n",
    "    # Test input sanitization\n",
    "    malicious_input = \"<script>alert('xss')</script>Hello world\"\n",
    "    sanitized = security_manager.sanitize_input(malicious_input)\n",
    "    print(f\"🛡️ Input Sanitization: {'✅ PASSED' if '[BLOCKED]' in sanitized else '❌ FAILED'}\")\n",
    "    \n",
    "    # Test API key validation\n",
    "    valid_key = security_manager.validate_api_key(\"demo-key-123\")\n",
    "    invalid_key = security_manager.validate_api_key(\"invalid-key\")\n",
    "    print(f\"🔑 API Key Validation: {'✅ PASSED' if valid_key and not invalid_key else '❌ FAILED'}\")\n",
    "    \n",
    "    # Test encryption\n",
    "    test_data = \"sensitive information\"\n",
    "    encrypted = security_manager.encrypt_data(test_data)\n",
    "    decrypted = security_manager.decrypt_data(encrypted)\n",
    "    print(f\"🔐 Encryption: {'✅ PASSED' if decrypted == test_data else '❌ FAILED'}\")\n",
    "    \n",
    "    # Test 4: Error Handling and Recovery\n",
    "    print(\"\\n4️⃣ Testing Error Handling\")\n",
    "    \n",
    "    error_state = BusinessWorkflowState(\n",
    "        user_id=\"test-user-3\",\n",
    "        workflow_type=\"customer_support\",\n",
    "        messages=[HumanMessage(content=\"\")]  # Empty message to trigger error\n",
    "    )\n",
    "    \n",
    "    config3 = {\"configurable\": {\"thread_id\": \"test-error-1\"}}\n",
    "    \n",
    "    try:\n",
    "        result3 = production_graph.invoke(error_state, config=config3)\n",
    "        has_errors = len(result3.errors) > 0 or result3.status == \"failed\"\n",
    "        print(f\"🚨 Error Handling: {'✅ PASSED' if has_errors else '✅ NO ERRORS'}\")\n",
    "        if result3.errors:\n",
    "            print(f\"📋 Error Count: {len(result3.errors)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"🚨 Error Handling: ✅ PASSED (Exception caught: {type(e).__name__})\")\n",
    "    \n",
    "    # Test 5: Performance and Monitoring\n",
    "    print(\"\\n5️⃣ Testing Performance Monitoring\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    perf_state = BusinessWorkflowState(\n",
    "        user_id=\"test-user-4\",\n",
    "        workflow_type=\"customer_support\",\n",
    "        messages=[HumanMessage(content=\"Quick test message\")]\n",
    "    )\n",
    "    \n",
    "    config4 = {\"configurable\": {\"thread_id\": \"test-perf-1\"}}\n",
    "    \n",
    "    try:\n",
    "        result4 = production_graph.invoke(perf_state, config=config4)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"⏱️ Processing Time: {processing_time:.2f}s\")\n",
    "        print(f\"💰 Cost Estimate: ${result4.actual_cost:.4f}\")\n",
    "        print(f\"📊 Audit Trail Entries: {len(result4.audit_trail)}\")\n",
    "        print(f\"⚡ Performance: {'✅ GOOD' if processing_time < 10 else '⚠️ SLOW'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Performance Test Failed: {e}\")\n",
    "    \n",
    "    print(\"\\n🎉 Production System Testing Complete!\")\n",
    "    print(\"📋 All core features tested and validated\")\n",
    "\n",
    "# Run comprehensive tests\n",
    "await test_production_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🛠️ Final Challenge (30 minutes)\n",
    "\n",
    "### Build Your Own Production Multi-Agent System\n",
    "\n",
    "**Goal**: Create a complete, production-ready multi-agent system for a real business use case.\n",
    "\n",
    "**Choose Your Use Case**:\n",
    "1. **E-commerce Assistant**: Product recommendations, order processing, customer support\n",
    "2. **HR Automation**: Resume screening, interview scheduling, onboarding workflows\n",
    "3. **Financial Advisor**: Portfolio analysis, risk assessment, investment recommendations\n",
    "4. **Educational Platform**: Course recommendations, progress tracking, personalized learning\n",
    "5. **Healthcare Assistant**: Symptom analysis, appointment scheduling, care coordination\n",
    "\n",
    "**Requirements**:\n",
    "- Multi-agent workflow with at least 3 specialized agents\n",
    "- Complete API endpoints with authentication\n",
    "- Error handling and recovery mechanisms\n",
    "- Cost monitoring and optimization\n",
    "- Production deployment configuration\n",
    "- Security and compliance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Challenge: Your Implementation Here\n",
    "\n",
    "# Step 1: Define your business domain\n",
    "class YourBusinessState(BaseModel):\n",
    "    \"\"\"Define your custom business state\"\"\"\n",
    "    # TODO: Add your domain-specific fields\n",
    "    pass\n",
    "\n",
    "# Step 2: Create specialized agents\n",
    "def your_first_agent(state: YourBusinessState) -> YourBusinessState:\n",
    "    \"\"\"Your first specialized agent\"\"\"\n",
    "    # TODO: Implement your first agent logic\n",
    "    pass\n",
    "\n",
    "def your_second_agent(state: YourBusinessState) -> YourBusinessState:\n",
    "    \"\"\"Your second specialized agent\"\"\"\n",
    "    # TODO: Implement your second agent logic\n",
    "    pass\n",
    "\n",
    "def your_third_agent(state: YourBusinessState) -> YourBusinessState:\n",
    "    \"\"\"Your third specialized agent\"\"\"\n",
    "    # TODO: Implement your third agent logic\n",
    "    pass\n",
    "\n",
    "# Step 3: Create your production graph\n",
    "def create_your_production_system():\n",
    "    \"\"\"Create your complete production system\"\"\"\n",
    "    # TODO: Build your graph with routing logic\n",
    "    pass\n",
    "\n",
    "# Step 4: Add API endpoints\n",
    "# TODO: Create FastAPI endpoints for your system\n",
    "\n",
    "# Step 5: Test your system\n",
    "def test_your_system():\n",
    "    \"\"\"Test your production system\"\"\"\n",
    "    # TODO: Create comprehensive tests\n",
    "    pass\n",
    "\n",
    "print(\"🎯 Final Challenge: Build your own production multi-agent system!\")\n",
    "print(\"💡 Follow the examples above and create something amazing!\")\n",
    "print(\"🚀 Remember: Think big, start simple, iterate quickly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎓 Course Completion & Next Steps\n",
    "\n",
    "### 🏆 Congratulations! You've Mastered LangGraph Multi-Agent Systems\n",
    "\n",
    "Over the past 7 days, you've learned:\n",
    "\n",
    "#### **Day 1**: Foundations & Type-Safe Development\n",
    "✅ Environment setup and Pydantic integration  \n",
    "✅ Structured outputs with OpenAI function calling  \n",
    "✅ Type-safe agent development patterns  \n",
    "\n",
    "#### **Day 2**: State Management & Persistence\n",
    "✅ Graph architecture and conditional routing  \n",
    "✅ SQLite and PostgreSQL checkpointing  \n",
    "✅ Error handling and recovery mechanisms  \n",
    "\n",
    "#### **Day 3**: Memory Systems & Knowledge Management\n",
    "✅ Semantic, episodic, and procedural memory  \n",
    "✅ Vector storage with OpenAI embeddings  \n",
    "✅ Cross-session knowledge retention  \n",
    "\n",
    "#### **Day 4**: Multi-Agent Communication & Handoffs\n",
    "✅ Agent handoffs and Command objects  \n",
    "✅ Supervisor and network patterns  \n",
    "✅ Secure multi-agent communication  \n",
    "\n",
    "#### **Day 5**: Advanced Architectures & Tool Integration\n",
    "✅ Hierarchical multi-agent systems  \n",
    "✅ Parallel execution and map-reduce  \n",
    "✅ External tool integration and optimization  \n",
    "\n",
    "#### **Day 6**: Production Tools & Monitoring\n",
    "✅ LangSmith observability and LangGraph Studio  \n",
    "✅ PostgreSQL production persistence  \n",
    "✅ Human-in-the-loop workflows and cost monitoring  \n",
    "\n",
    "#### **Day 7**: Deployment & Real-World Applications\n",
    "✅ Production deployment with Docker and Kubernetes  \n",
    "✅ Security, authentication, and rate limiting  \n",
    "✅ Complete business workflow implementations  \n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Your Next Steps\n",
    "\n",
    "#### **Immediate Actions** (Next 1-2 weeks)\n",
    "1. **Deploy Your First System**: Take one of the examples and deploy it to production\n",
    "2. **Build a Real Project**: Apply these concepts to a real business problem\n",
    "3. **Join the Community**: Connect with other LangGraph developers\n",
    "4. **Share Your Work**: Contribute examples and share your experiences\n",
    "\n",
    "#### **Advanced Learning** (Next 1-3 months)\n",
    "1. **LangGraph Platform**: Explore the managed hosting platform\n",
    "2. **Custom Tools**: Build domain-specific tools and integrations\n",
    "3. **Advanced Patterns**: Research cutting-edge multi-agent architectures\n",
    "4. **Performance Optimization**: Master cost and performance optimization\n",
    "\n",
    "#### **Community & Resources**\n",
    "- **LangChain Community**: [Discord](https://discord.gg/langchain) and [Forum](https://community.langchain.com/)\n",
    "- **GitHub**: Contribute to [LangGraph](https://github.com/langchain-ai/langgraph)\n",
    "- **Documentation**: Stay updated with [Official Docs](https://langchain-ai.github.io/langgraph/)\n",
    "- **Blog**: Follow [LangChain Blog](https://blog.langchain.com/) for updates\n",
    "\n",
    "---\n",
    "\n",
    "### 💼 Business Applications to Explore\n",
    "\n",
    "**High-Impact Use Cases**:\n",
    "- Customer support automation with escalation\n",
    "- Content generation and review workflows\n",
    "- Data analysis and reporting pipelines\n",
    "- Process automation and decision support\n",
    "- Quality assurance and compliance checking\n",
    "\n",
    "**Industry Applications**:\n",
    "- **Financial Services**: Fraud detection, risk assessment, compliance\n",
    "- **Healthcare**: Patient triage, care coordination, documentation\n",
    "- **E-commerce**: Product recommendations, order processing, support\n",
    "- **Education**: Personalized learning, assessment, curriculum design\n",
    "- **Manufacturing**: Quality control, predictive maintenance, optimization\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Success Metrics for Your Journey\n",
    "\n",
    "**Technical Milestones**:\n",
    "- ✅ Built and deployed your first multi-agent system\n",
    "- ✅ Implemented production monitoring and alerting\n",
    "- ✅ Achieved target performance and cost metrics\n",
    "- ✅ Scaled system to handle production traffic\n",
    "\n",
    "**Business Impact**:\n",
    "- 📈 Measured ROI from automation initiatives\n",
    "- 🎯 Improved customer satisfaction scores\n",
    "- ⏱️ Reduced processing times for key workflows\n",
    "- 💰 Optimized operational costs through automation\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 Final Words\n",
    "\n",
    "You now have the knowledge and tools to build production-ready multi-agent systems that can transform businesses and solve real-world problems. The future of AI is multi-agent, and you're now equipped to be part of that future.\n",
    "\n",
    "**Remember**:\n",
    "- Start with simple, working systems and iterate\n",
    "- Focus on solving real problems, not just implementing technology\n",
    "- Monitor, measure, and optimize continuously\n",
    "- Share your knowledge and learn from the community\n",
    "\n",
    "**🚀 Go build something amazing!**\n",
    "\n",
    "---\n",
    "\n",
    "*Thank you for completing the 7-Day LangGraph Multi-Agent Systems Course. You're now ready to build the future of AI applications!* 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course completion celebration!\n",
    "def course_completion_summary():\n",
    "    \"\"\"Generate course completion summary\"\"\"\n",
    "    \n",
    "    print(\"🎉\" * 50)\n",
    "    print(\"🏆 COURSE COMPLETED SUCCESSFULLY! 🏆\")\n",
    "    print(\"🎉\" * 50)\n",
    "    \n",
    "    skills_learned = [\n",
    "        \"✅ Multi-agent system architecture\",\n",
    "        \"✅ Production deployment strategies\",\n",
    "        \"✅ Security and authentication\",\n",
    "        \"✅ Monitoring and observability\",\n",
    "        \"✅ Cost optimization techniques\",\n",
    "        \"✅ Error handling and recovery\",\n",
    "        \"✅ Business workflow automation\",\n",
    "        \"✅ Real-world application development\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n📚 Skills You've Mastered:\")\n",
    "    for skill in skills_learned:\n",
    "        print(f\"  {skill}\")\n",
    "    \n",
    "    next_steps = [\n",
    "        \"🚀 Deploy your first production system\",\n",
    "        \"🏗️ Build a real business application\",\n",
    "        \"🤝 Join the LangGraph community\",\n",
    "        \"📖 Explore advanced patterns and optimizations\",\n",
    "        \"💼 Apply for roles in AI/ML engineering\",\n",
    "        \"🌟 Share your knowledge and help others\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🎯 Your Next Steps:\")\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "    \n",
    "    print(\"\\n💡 Remember: The best way to learn is by building!\")\n",
    "    print(\"🌟 You now have all the tools to create amazing AI systems.\")\n",
    "    print(\"🚀 Go forth and build the future!\")\n",
    "    \n",
    "    return {\n",
    "        \"course_completed\": True,\n",
    "        \"skills_count\": len(skills_learned),\n",
    "        \"ready_for_production\": True,\n",
    "        \"confidence_level\": \"Expert\"\n",
    "    }\n",
    "\n",
    "# Generate completion summary\n",
    "completion_stats = course_completion_summary()\n",
    "\n",
    "print(f\"\\n📊 Course Statistics:\")\n",
    "print(f\"  📅 Days Completed: 7/7\")\n",
    "print(f\"  🎯 Skills Mastered: {completion_stats['skills_count']}\")\n",
    "print(f\"  🏭 Production Ready: {completion_stats['ready_for_production']}\")\n",
    "print(f\"  💪 Confidence Level: {completion_stats['confidence_level']}\")\n",
    "\n",
    "print(\"\\n🎓 Congratulations on completing the LangGraph Multi-Agent Systems Course!\")\n",
    "print(\"👏 You are now a certified LangGraph expert!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"  },  "language_info": {  "codemirror_mode": {    "name": "ipython",    "version": 3   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython3",  "version": "3.8.0"  } }, "nbformat": 4, "nbformat_minor": 4}