{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5: Advanced Architectures & Tool Integration in LangGraph\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- Build hierarchical multi-agent systems with complex organizational structures\n",
    "- Implement parallel execution and map-reduce patterns for scalable processing\n",
    "- Integrate external tools (Tavily search, APIs) with OpenAI for enhanced capabilities\n",
    "- Optimize performance and reduce costs in LangGraph applications\n",
    "- Deploy production-ready multi-agent systems\n",
    "- Master advanced architectural patterns and best practices\n",
    "\n",
    "## ⏱️ Session Structure (2 hours)\n",
    "- **Learning Materials** (30 min): Advanced patterns and architecture theory\n",
    "- **Hands-on Code** (60 min): Implementation and tool integration  \n",
    "- **Practical Exercises** (30 min): Build scalable production systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📖 Learning Materials (30 minutes)\n",
    "\n",
    "### 📺 Video Resources\n",
    "- [LangGraph Advanced Patterns](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) - Agentic concepts and patterns\n",
    "- [DeepLearning.AI - AI Agents in LangGraph](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/) - Module 5: Advanced Architectures\n",
    "- [LangChain Academy - Production Deployment](https://academy.langchain.com/) - Scaling and optimization\n",
    "\n",
    "### 🧠 Theory: Advanced Architectural Patterns\n",
    "\n",
    "#### Hierarchical Multi-Agent Systems\n",
    "Hierarchical systems organize agents in layers with clear command structures:\n",
    "- **Executive Layer**: High-level decision making and strategy\n",
    "- **Management Layer**: Task coordination and resource allocation\n",
    "- **Worker Layer**: Specialized task execution\n",
    "- **Support Layer**: Shared services and utilities\n",
    "\n",
    "#### Parallel Execution Patterns\n",
    "1. **Map-Reduce**: Distribute work across multiple agents and aggregate results\n",
    "2. **Pipeline Parallelism**: Different stages of processing run concurrently\n",
    "3. **Data Parallelism**: Same operation on different data partitions\n",
    "4. **Task Parallelism**: Different operations running simultaneously\n",
    "\n",
    "#### Tool Integration Architecture\n",
    "- **Tool Binding**: Direct integration with LangChain tools\n",
    "- **API Orchestration**: Managing external service calls\n",
    "- **Error Handling**: Resilient tool usage with fallbacks\n",
    "- **Caching**: Optimizing repeated tool calls\n",
    "- **Rate Limiting**: Respecting API constraints\n",
    "\n",
    "#### Performance Optimization Strategies\n",
    "1. **Model Selection**: Choosing appropriate models for different tasks\n",
    "2. **Prompt Engineering**: Efficient prompts that reduce token usage\n",
    "3. **Caching**: State and response caching\n",
    "4. **Batching**: Grouping operations for efficiency\n",
    "5. **Streaming**: Real-time response processing\n",
    "\n",
    "#### Cost Optimization for OpenAI\n",
    "- **Model Tiering**: Use GPT-3.5-turbo for simple tasks, GPT-4 for complex ones\n",
    "- **Token Management**: Minimize input/output tokens\n",
    "- **Response Filtering**: Stop generation early when possible\n",
    "- **Batch Processing**: Combine multiple requests\n",
    "- **Smart Retries**: Avoid unnecessary API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 💻 Hands-on Code (60 minutes)\n",
    "\n",
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langgraph langchain langchain-openai pydantic python-dotenv\n",
    "!pip install langgraph-checkpoint-sqlite httpx aiohttp\n",
    "!pip install tavily-python  # For web search integration\n",
    "!pip install tiktoken  # For token counting\n",
    "!pip install asyncio-throttle  # For rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from typing import TypedDict, Literal, List, Optional, Dict, Any, Union\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.types import Command\n",
    "from langgraph.constants import INTERRUPT\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# Tool imports\n",
    "from tavily import TavilyClient\n",
    "import httpx\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure APIs\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"⚠️ Please set OPENAI_API_KEY in your .env file\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key loaded successfully\")\n",
    "\n",
    "if not tavily_api_key:\n",
    "    print(\"⚠️ Please set TAVILY_API_KEY in your .env file for web search\")\n",
    "    print(\"Get your key at: https://tavily.com/\")\n",
    "else:\n",
    "    print(\"✅ Tavily API key loaded successfully\")\n",
    "\n",
    "# Initialize models with different configurations\n",
    "executive_llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1, openai_api_key=openai_api_key)  # Strategic decisions\n",
    "manager_llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key=openai_api_key)    # Planning and coordination\n",
    "worker_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)  # Task execution\n",
    "fast_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=200, openai_api_key=openai_api_key)  # Quick tasks\n",
    "\n",
    "# Initialize Tavily client\n",
    "tavily_client = TavilyClient(api_key=tavily_api_key) if tavily_api_key else None\n",
    "\n",
    "print(\"🚀 Advanced LangGraph setup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hierarchical Multi-Agent Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentLevel(str, Enum):\n",
    "    \"\"\"Hierarchical levels in the organization\"\"\"\n",
    "    EXECUTIVE = \"executive\"\n",
    "    MANAGER = \"manager\"\n",
    "    WORKER = \"worker\"\n",
    "    SUPPORT = \"support\"\n",
    "\n",
    "class TaskType(str, Enum):\n",
    "    \"\"\"Types of tasks in the system\"\"\"\n",
    "    STRATEGIC = \"strategic\"\n",
    "    OPERATIONAL = \"operational\"\n",
    "    EXECUTION = \"execution\"\n",
    "    ANALYSIS = \"analysis\"\n",
    "    RESEARCH = \"research\"\n",
    "\n",
    "class Task(BaseModel):\n",
    "    \"\"\"Task definition in hierarchical system\"\"\"\n",
    "    task_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    task_type: TaskType\n",
    "    priority: int = Field(ge=1, le=5)  # 1 = highest priority\n",
    "    assigned_level: AgentLevel\n",
    "    assigned_agent: Optional[str] = None\n",
    "    dependencies: List[str] = Field(default_factory=list)\n",
    "    status: str = \"pending\"\n",
    "    result: Optional[str] = None\n",
    "    created_at: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
    "    completed_at: Optional[str] = None\n",
    "    estimated_tokens: int = 0\n",
    "    actual_tokens: int = 0\n",
    "\n",
    "class HierarchicalState(BaseModel):\n",
    "    \"\"\"State for hierarchical multi-agent system\"\"\"\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    tasks: List[Task] = Field(default_factory=list)\n",
    "    completed_tasks: List[Task] = Field(default_factory=list)\n",
    "    current_task: Optional[Task] = None\n",
    "    strategic_plan: Dict[str, Any] = Field(default_factory=dict)\n",
    "    operational_plan: Dict[str, Any] = Field(default_factory=dict)\n",
    "    execution_results: Dict[str, Any] = Field(default_factory=dict)\n",
    "    agent_workload: Dict[str, int] = Field(default_factory=dict)\n",
    "    performance_metrics: Dict[str, Any] = Field(default_factory=dict)\n",
    "    total_tokens_used: int = 0\n",
    "    estimated_cost: float = 0.0\n",
    "\n",
    "# Token counting utility\n",
    "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Count tokens in text for cost estimation\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        return len(encoding.encode(text))\n",
    "    except:\n",
    "        # Fallback estimation\n",
    "        return len(text) // 4\n",
    "\n",
    "def estimate_cost(tokens: int, model: str = \"gpt-3.5-turbo\") -> float:\n",
    "    \"\"\"Estimate cost based on token usage\"\"\"\n",
    "    # Current OpenAI pricing (approximate)\n",
    "    pricing = {\n",
    "        \"gpt-3.5-turbo\": 0.0015 / 1000,  # $0.0015 per 1K tokens\n",
    "        \"gpt-4\": 0.03 / 1000,            # $0.03 per 1K tokens\n",
    "    }\n",
    "    rate = pricing.get(model, pricing[\"gpt-3.5-turbo\"])\n",
    "    return tokens * rate\n",
    "\n",
    "# Executive layer - Strategic planning\n",
    "def executive_agent(state: HierarchicalState) -> Command:\n",
    "    \"\"\"Executive agent handles strategic planning and high-level decisions\"\"\"\n",
    "    print(\"👔 Executive: Developing strategic plan\")\n",
    "    \n",
    "    # Analyze incoming tasks and create strategic plan\n",
    "    pending_tasks = [t for t in state.tasks if t.status == \"pending\"]\n",
    "    \n",
    "    if pending_tasks:\n",
    "        tasks_summary = \"\\n\".join([f\"- {t.title}: {t.description}\" for t in pending_tasks])\n",
    "        \n",
    "        strategic_prompt = f\"\"\"\n",
    "        You are the Executive Agent responsible for strategic planning.\n",
    "        \n",
    "        Analyze these pending tasks and create a strategic plan:\n",
    "        {tasks_summary}\n",
    "        \n",
    "        Provide:\n",
    "        1. Overall strategy and approach\n",
    "        2. Priority ordering of tasks\n",
    "        3. Resource allocation recommendations\n",
    "        4. Risk assessment\n",
    "        5. Success metrics\n",
    "        \n",
    "        Be concise but comprehensive.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Count tokens for cost tracking\n",
    "        input_tokens = count_tokens(strategic_prompt, \"gpt-4\")\n",
    "        \n",
    "        response = executive_llm.invoke([HumanMessage(content=strategic_prompt)])\n",
    "        \n",
    "        output_tokens = count_tokens(response.content, \"gpt-4\")\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        state.total_tokens_used += total_tokens\n",
    "        state.estimated_cost += estimate_cost(total_tokens, \"gpt-4\")\n",
    "        \n",
    "        # Store strategic plan\n",
    "        state.strategic_plan = {\n",
    "            \"plan\": response.content,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"agent\": \"executive\",\n",
    "            \"tokens_used\": total_tokens\n",
    "        }\n",
    "        \n",
    "        state.messages.append(response)\n",
    "        \n",
    "        print(f\"📊 Executive: Strategic plan created (Tokens: {total_tokens}, Cost: ${state.estimated_cost:.4f})\")\n",
    "        \n",
    "        return Command(goto=\"manager\", update=state.model_dump())\n",
    "    \n",
    "    return Command(goto=\"manager\", update=state.model_dump())\n",
    "\n",
    "# Manager layer - Operational planning\n",
    "def manager_agent(state: HierarchicalState) -> Command:\n",
    "    \"\"\"Manager agent creates operational plans and coordinates execution\"\"\"\n",
    "    print(\"📋 Manager: Creating operational plan\")\n",
    "    \n",
    "    pending_tasks = [t for t in state.tasks if t.status == \"pending\"]\n",
    "    \n",
    "    if pending_tasks and state.strategic_plan:\n",
    "        operational_prompt = f\"\"\"\n",
    "        You are the Manager Agent responsible for operational planning.\n",
    "        \n",
    "        Strategic Plan:\n",
    "        {state.strategic_plan.get('plan', 'No strategic plan available')}\n",
    "        \n",
    "        Tasks to execute:\n",
    "        {[t.title for t in pending_tasks]}\n",
    "        \n",
    "        Create an operational plan with:\n",
    "        1. Task breakdown and sequencing\n",
    "        2. Agent assignment recommendations\n",
    "        3. Timeline and milestones\n",
    "        4. Resource requirements\n",
    "        5. Quality checkpoints\n",
    "        \n",
    "        Focus on practical execution steps.\n",
    "        \"\"\"\n",
    "        \n",
    "        input_tokens = count_tokens(operational_prompt, \"gpt-4\")\n",
    "        response = manager_llm.invoke([HumanMessage(content=operational_prompt)])\n",
    "        output_tokens = count_tokens(response.content, \"gpt-4\")\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        state.total_tokens_used += total_tokens\n",
    "        state.estimated_cost += estimate_cost(total_tokens, \"gpt-4\")\n",
    "        \n",
    "        # Store operational plan\n",
    "        state.operational_plan = {\n",
    "            \"plan\": response.content,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"agent\": \"manager\",\n",
    "            \"tokens_used\": total_tokens\n",
    "        }\n",
    "        \n",
    "        state.messages.append(response)\n",
    "        \n",
    "        # Assign tasks to workers based on operational plan\n",
    "        for task in pending_tasks:\n",
    "            if task.task_type in [TaskType.EXECUTION, TaskType.ANALYSIS, TaskType.RESEARCH]:\n",
    "                task.assigned_level = AgentLevel.WORKER\n",
    "                task.status = \"assigned\"\n",
    "        \n",
    "        print(f\"📊 Manager: Operational plan created (Tokens: {total_tokens}, Cost: ${state.estimated_cost:.4f})\")\n",
    "        \n",
    "        return Command(goto=\"worker_coordinator\", update=state.model_dump())\n",
    "    \n",
    "    return Command(goto=\"worker_coordinator\", update=state.model_dump())\n",
    "\n",
    "# Worker coordinator - Manages parallel execution\n",
    "def worker_coordinator(state: HierarchicalState) -> Command:\n",
    "    \"\"\"Coordinates parallel execution of worker tasks\"\"\"\n",
    "    print(\"👥 Worker Coordinator: Orchestrating parallel execution\")\n",
    "    \n",
    "    assigned_tasks = [t for t in state.tasks if t.status == \"assigned\"]\n",
    "    \n",
    "    if assigned_tasks:\n",
    "        # Process tasks in parallel (simulated)\n",
    "        for task in assigned_tasks:\n",
    "            if task.task_type == TaskType.RESEARCH:\n",
    "                task.assigned_agent = \"research_worker\"\n",
    "            elif task.task_type == TaskType.ANALYSIS:\n",
    "                task.assigned_agent = \"analysis_worker\"\n",
    "            else:\n",
    "                task.assigned_agent = \"general_worker\"\n",
    "            \n",
    "            task.status = \"in_progress\"\n",
    "        \n",
    "        print(f\"⚡ Coordinator: {len(assigned_tasks)} tasks distributed to workers\")\n",
    "        \n",
    "        # Start with first task (in real implementation, would be parallel)\n",
    "        if assigned_tasks:\n",
    "            state.current_task = assigned_tasks[0]\n",
    "            \n",
    "            if state.current_task.task_type == TaskType.RESEARCH:\n",
    "                return Command(goto=\"research_worker\", update=state.model_dump())\n",
    "            elif state.current_task.task_type == TaskType.ANALYSIS:\n",
    "                return Command(goto=\"analysis_worker\", update=state.model_dump())\n",
    "            else:\n",
    "                return Command(goto=\"general_worker\", update=state.model_dump())\n",
    "    \n",
    "    return Command(goto=\"results_aggregator\", update=state.model_dump())\n",
    "\n",
    "print(\"🏗️ Hierarchical agent architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tool Integration with Tavily and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool integration utilities\n",
    "class ToolResult(BaseModel):\n",
    "    \"\"\"Result from tool execution\"\"\"\n",
    "    tool_name: str\n",
    "    success: bool\n",
    "    result: Any\n",
    "    error: Optional[str] = None\n",
    "    execution_time: float\n",
    "    tokens_used: int = 0\n",
    "    cost_estimate: float = 0.0\n",
    "\n",
    "class ToolManager:\n",
    "    \"\"\"Manages tool integration and caching\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.rate_limits = {\n",
    "            \"tavily\": {\"calls_per_minute\": 100, \"last_calls\": []},\n",
    "            \"openai\": {\"calls_per_minute\": 3000, \"last_calls\": []}\n",
    "        }\n",
    "    \n",
    "    def _check_rate_limit(self, tool_name: str) -> bool:\n",
    "        \"\"\"Check if tool can be called without hitting rate limits\"\"\"\n",
    "        if tool_name not in self.rate_limits:\n",
    "            return True\n",
    "        \n",
    "        rate_info = self.rate_limits[tool_name]\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Remove calls older than 1 minute\n",
    "        rate_info[\"last_calls\"] = [\n",
    "            call_time for call_time in rate_info[\"last_calls\"]\n",
    "            if current_time - call_time < 60\n",
    "        ]\n",
    "        \n",
    "        # Check if we can make another call\n",
    "        return len(rate_info[\"last_calls\"]) < rate_info[\"calls_per_minute\"]\n",
    "    \n",
    "    def _record_call(self, tool_name: str):\n",
    "        \"\"\"Record a tool call for rate limiting\"\"\"\n",
    "        if tool_name in self.rate_limits:\n",
    "            self.rate_limits[tool_name][\"last_calls\"].append(time.time())\n",
    "    \n",
    "    def search_web(self, query: str, max_results: int = 5) -> ToolResult:\n",
    "        \"\"\"Search the web using Tavily\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = f\"tavily_{query}_{max_results}\"\n",
    "        if cache_key in self.cache:\n",
    "            print(f\"🔍 Tavily: Using cached result for '{query}'\")\n",
    "            cached_result = self.cache[cache_key]\n",
    "            cached_result.execution_time = time.time() - start_time\n",
    "            return cached_result\n",
    "        \n",
    "        # Check rate limits\n",
    "        if not self._check_rate_limit(\"tavily\"):\n",
    "            return ToolResult(\n",
    "                tool_name=\"tavily\",\n",
    "                success=False,\n",
    "                result=None,\n",
    "                error=\"Rate limit exceeded\",\n",
    "                execution_time=time.time() - start_time\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            if not tavily_client:\n",
    "                return ToolResult(\n",
    "                    tool_name=\"tavily\",\n",
    "                    success=False,\n",
    "                    result=None,\n",
    "                    error=\"Tavily client not configured\",\n",
    "                    execution_time=time.time() - start_time\n",
    "                )\n",
    "            \n",
    "            print(f\"🔍 Tavily: Searching for '{query}'\")\n",
    "            \n",
    "            # Perform search\n",
    "            response = tavily_client.search(\n",
    "                query=query,\n",
    "                search_depth=\"basic\",\n",
    "                max_results=max_results\n",
    "            )\n",
    "            \n",
    "            self._record_call(\"tavily\")\n",
    "            \n",
    "            # Process results\n",
    "            processed_results = []\n",
    "            for result in response.get(\"results\", []):\n",
    "                processed_results.append({\n",
    "                    \"title\": result.get(\"title\", \"\"),\n",
    "                    \"url\": result.get(\"url\", \"\"),\n",
    "                    \"content\": result.get(\"content\", \"\")[:500],  # Limit content length\n",
    "                    \"score\": result.get(\"score\", 0)\n",
    "                })\n",
    "            \n",
    "            result = ToolResult(\n",
    "                tool_name=\"tavily\",\n",
    "                success=True,\n",
    "                result=processed_results,\n",
    "                execution_time=time.time() - start_time\n",
    "            )\n",
    "            \n",
    "            # Cache successful results\n",
    "            self.cache[cache_key] = result\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ToolResult(\n",
    "                tool_name=\"tavily\",\n",
    "                success=False,\n",
    "                result=None,\n",
    "                error=str(e),\n",
    "                execution_time=time.time() - start_time\n",
    "            )\n",
    "    \n",
    "    def call_api(self, url: str, method: str = \"GET\", data: Dict = None) -> ToolResult:\n",
    "        \"\"\"Make API calls with error handling\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            print(f\"🌐 API: {method} request to {url}\")\n",
    "            \n",
    "            with httpx.Client(timeout=10) as client:\n",
    "                if method.upper() == \"GET\":\n",
    "                    response = client.get(url)\n",
    "                elif method.upper() == \"POST\":\n",
    "                    response = client.post(url, json=data)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported method: {method}\")\n",
    "                \n",
    "                response.raise_for_status()\n",
    "                \n",
    "                return ToolResult(\n",
    "                    tool_name=\"api_call\",\n",
    "                    success=True,\n",
    "                    result=response.json() if response.content else None,\n",
    "                    execution_time=time.time() - start_time\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            return ToolResult(\n",
    "                tool_name=\"api_call\",\n",
    "                success=False,\n",
    "                result=None,\n",
    "                error=str(e),\n",
    "                execution_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "# Initialize tool manager\n",
    "tool_manager = ToolManager()\n",
    "\n",
    "# Research worker with tool integration\n",
    "def research_worker(state: HierarchicalState) -> Command:\n",
    "    \"\"\"Research worker that uses Tavily for web search\"\"\"\n",
    "    print(\"🔬 Research Worker: Conducting research with tools\")\n",
    "    \n",
    "    if state.current_task:\n",
    "        task = state.current_task\n",
    "        \n",
    "        # Extract search queries from task description\n",
    "        query_extraction_prompt = f\"\"\"\n",
    "        Extract 2-3 specific search queries from this research task:\n",
    "        Task: {task.title}\n",
    "        Description: {task.description}\n",
    "        \n",
    "        Return only the search queries, one per line, without any additional text.\n",
    "        \"\"\"\n",
    "        \n",
    "        input_tokens = count_tokens(query_extraction_prompt, \"gpt-3.5-turbo\")\n",
    "        response = fast_llm.invoke([HumanMessage(content=query_extraction_prompt)])\n",
    "        output_tokens = count_tokens(response.content, \"gpt-3.5-turbo\")\n",
    "        \n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        state.total_tokens_used += total_tokens\n",
    "        state.estimated_cost += estimate_cost(total_tokens, \"gpt-3.5-turbo\")\n",
    "        \n",
    "        # Extract queries\n",
    "        queries = [q.strip() for q in response.content.split('\\n') if q.strip()]\n",
    "        \n",
    "        # Perform web searches\n",
    "        search_results = []\n",
    "        for query in queries[:3]:  # Limit to 3 queries\n",
    "            result = tool_manager.search_web(query, max_results=3)\n",
    "            if result.success:\n",
    "                search_results.extend(result.result)\n",
    "            else:\n",
    "                print(f\"⚠️ Search failed for '{query}': {result.error}\")\n",
    "        \n",
    "        # Synthesize research findings\n",
    "        if search_results:\n",
    "            research_content = \"\\n\\n\".join([\n",
    "                f\"Title: {r['title']}\\nURL: {r['url']}\\nContent: {r['content']}\"\n",
    "                for r in search_results[:5]  # Limit to top 5 results\n",
    "            ])\n",
    "            \n",
    "            synthesis_prompt = f\"\"\"\n",
    "            You are a research specialist. Synthesize these web search results for the task:\n",
    "            \n",
    "            Task: {task.title}\n",
    "            Description: {task.description}\n",
    "            \n",
    "            Search Results:\n",
    "            {research_content[:3000]}  # Limit content to manage tokens\n",
    "            \n",
    "            Provide a comprehensive research summary with key findings, insights, and sources.\n",
    "            \"\"\"\n",
    "            \n",
    "            input_tokens = count_tokens(synthesis_prompt, \"gpt-3.5-turbo\")\n",
    "            synthesis_response = worker_llm.invoke([HumanMessage(content=synthesis_prompt)])\n",
    "            output_tokens = count_tokens(synthesis_response.content, \"gpt-3.5-turbo\")\n",
    "            \n",
    "            total_tokens += input_tokens + output_tokens\n",
    "            state.total_tokens_used += total_tokens\n",
    "            state.estimated_cost += estimate_cost(total_tokens, \"gpt-3.5-turbo\")\n",
    "            \n",
    "            task.result = synthesis_response.content\n",
    "            task.actual_tokens = total_tokens\n",
    "            \n",
    "        else:\n",
    "            # Fallback to knowledge-based research\n",
    "            fallback_prompt = f\"\"\"\n",
    "            Conduct research on: {task.title}\n",
    "            Description: {task.description}\n",
    "            \n",
    "            Provide comprehensive research findings based on your knowledge.\n",
    "            \"\"\"\n",
    "            \n",
    "            fallback_response = worker_llm.invoke([HumanMessage(content=fallback_prompt)])\n",
    "            task.result = fallback_response.content\n",
    "        \n",
    "        task.status = \"completed\"\n",
    "        task.completed_at = datetime.now().isoformat()\n",
    "        \n",
    "        # Move to completed tasks\n",
    "        state.completed_tasks.append(task)\n",
    "        state.tasks = [t for t in state.tasks if t.task_id != task.task_id]\n",
    "        state.current_task = None\n",
    "        \n",
    "        print(f\"✅ Research completed (Tokens: {task.actual_tokens}, Cost: ${state.estimated_cost:.4f})\")\n",
    "    \n",
    "    return Command(goto=\"worker_coordinator\", update=state.model_dump())\n",
    "\n",
    "# Analysis worker\n",
    "def analysis_worker(state: HierarchicalState) -> Command:\n",
    "    \"\"\"Analysis worker for data processing and insights\"\"\"\n",
    "    print(\"📊 Analysis Worker: Processing and analyzing data\")\n",
    "    \n",
    "    if state.current_task:\n",
    "        task = state.current_task\n",
    "        \n",
    "        # Get data from completed research tasks\n",
    "        research_data = \"\\n\\n\".join([\n",
    "            f\"Research: {t.title}\\nFindings: {t.result}\"\n",
    "            for t in state.completed_tasks\n",
    "            if t.task_type == TaskType.RESEARCH and t.result\n",
    "        ])\n",
    "        \n",
    "        analysis_prompt = f\"\"\"\n",
    "        You are a data analysis specialist. Analyze the following for the task:\n",
    "        \n",
    "        Task: {task.title}\n",
    "        Description: {task.description}\n",
    "        \n",
    "        Available Research Data:\n",
    "        {research_data[:2000] if research_data else 'No research data available'}\n",
    "        \n",
    "        Provide detailed analysis with:\n",
    "        1. Key patterns and trends\n",
    "        2. Statistical insights\n",
    "        3. Comparative analysis\n",
    "        4. Actionable recommendations\n",
    "        5. Risk assessment\n",
    "        \"\"\"\n",
    "        \n",
    "        input_tokens = count_tokens(analysis_prompt, \"gpt-3.5-turbo\")\n",
    "        response = worker_llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "        output_tokens = count_tokens(response.content, \"gpt-3.5-turbo\")\n",
    "        \n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        state.total_tokens_used += total_tokens\n",
    "        state.estimated_cost += estimate_cost(total_tokens, \"gpt-3.5-turbo\")\n",
    "        \n",
    "        task.result = response.content\n",
    "        task.actual_tokens = total_tokens\n",
    "        task.status = \"completed\"\n",
    "        task.completed_at = datetime.now().isoformat()\n",
    "        \n",
    "        # Move to completed tasks\n",
    "        state.completed_tasks.append(task)\n",
    "        state.tasks = [t for t in state.tasks if t.task_id != task.task_id]\n",
    "        state.current_task = None\n",
    "        \n",
    "        print(f\"✅ Analysis completed (Tokens: {total_tokens}, Cost: ${state.estimated_cost:.4f})\")\n",
    "    \n",
    "    return Command(goto=\"worker_coordinator\", update=state.model_dump())\n",
    "\n",
    "print(\"🛠️ Tool-integrated workers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parallel Execution and Map-Reduce Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTask(BaseModel):\n",
    "    \"\"\"Task for parallel execution\"\"\"\n",
    "    task_id: str\n",
    "    data_chunk: Dict[str, Any]\n",
    "    operation: str\n",
    "    result: Optional[Dict[str, Any]] = None\n",
    "    status: str = \"pending\"\n",
    "    worker_id: Optional[str] = None\n",
    "    execution_time: Optional[float] = None\n",
    "    tokens_used: int = 0\n",
    "\n",
    "class MapReduceState(BaseModel):\n",
    "    \"\"\"State for map-reduce operations\"\"\"\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    input_data: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    parallel_tasks: List[ParallelTask] = Field(default_factory=list)\n",
    "    completed_tasks: List[ParallelTask] = Field(default_factory=list)\n",
    "    map_results: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    reduce_result: Optional[Dict[str, Any]] = None\n",
    "    operation_type: str = \"analysis\"\n",
    "    total_tokens_used: int = 0\n",
    "    estimated_cost: float = 0.0\n",
    "    performance_metrics: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "def map_coordinator(state: MapReduceState) -> Command:\n",
    "    \"\"\"Coordinates the map phase of parallel processing\"\"\"\n",
    "    print(\"🗺️ Map Coordinator: Distributing tasks for parallel processing\")\n",
    "    \n",
    "    # Create parallel tasks from input data\n",
    "    if state.input_data and not state.parallel_tasks:\n",
    "        for i, data_chunk in enumerate(state.input_data):\n",
    "            task = ParallelTask(\n",
    "                task_id=f\"map_task_{i}\",\n",
    "                data_chunk=data_chunk,\n",
    "                operation=state.operation_type,\n",
    "                worker_id=f\"worker_{i % 3}\"  # Distribute among 3 workers\n",
    "            )\n",
    "            state.parallel_tasks.append(task)\n",
    "        \n",
    "        print(f\"📊 Created {len(state.parallel_tasks)} parallel tasks\")\n",
    "    \n",
    "    # Process tasks (simulating parallel execution)\n",
    "    pending_tasks = [t for t in state.parallel_tasks if t.status == \"pending\"]\n",
    "    \n",
    "    if pending_tasks:\n",
    "        # Take first pending task\n",
    "        current_task = pending_tasks[0]\n",
    "        current_task.status = \"processing\"\n",
    "        \n",
    "        # Route to appropriate worker based on operation type\n",
    "        if state.operation_type == \"analysis\":\n",
    "            return Command(goto=\"map_analyzer\", update=state.model_dump())\n",
    "        elif state.operation_type == \"summarization\":\n",
    "            return Command(goto=\"map_summarizer\", update=state.model_dump())\n",
    "        else:\n",
    "            return Command(goto=\"map_processor\", update=state.model_dump())\n",
    "    \n",
    "    # All tasks completed, move to reduce phase\n",
    "    return Command(goto=\"reduce_aggregator\", update=state.model_dump())\n",
    "\n",
    "def map_analyzer(state: MapReduceState) -> Command:\n",
    "    \"\"\"Processes individual data chunks in parallel\"\"\"\n",
    "    print(\"🔍 Map Analyzer: Processing data chunk\")\n",
    "    \n",
    "    # Find current processing task\n",
    "    current_task = next((t for t in state.parallel_tasks if t.status == \"processing\"), None)\n",
    "    \n",
    "    if current_task:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Analyze the data chunk\n",
    "        data_content = json.dumps(current_task.data_chunk, indent=2)\n",
    "        \n",
    "        analysis_prompt = f\"\"\"\n",
    "        You are processing a data chunk in a parallel analysis system.\n",
    "        \n",
    "        Data chunk to analyze:\n",
    "        {data_content[:1000]}  # Limit content size\n",
    "        \n",
    "        Provide analysis with:\n",
    "        1. Key metrics and statistics\n",
    "        2. Notable patterns or anomalies\n",
    "        3. Summary insights\n",
    "        4. Quality score (1-10)\n",
    "        \n",
    "        Format as JSON with keys: metrics, patterns, insights, quality_score\n",
    "        \"\"\"\n",
    "        \n",
    "        input_tokens = count_tokens(analysis_prompt, \"gpt-3.5-turbo\")\n",
    "        response = worker_llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "        output_tokens = count_tokens(response.content, \"gpt-3.5-turbo\")\n",
    "        \n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        current_task.tokens_used = total_tokens\n",
    "        state.total_tokens_used += total_tokens\n",
    "        state.estimated_cost += estimate_cost(total_tokens, \"gpt-3.5-turbo\")\n",
    "        \n",
    "        # Parse result (with error handling)\n",
    "        try:\n",
    "            result_data = json.loads(response.content)\n",
    "        except json.JSONDecodeError:\n",
    "            result_data = {\n",
    "                \"metrics\": \"Error parsing response\",\n",
    "                \"patterns\": response.content[:200],\n",
    "                \"insights\": \"Failed to parse structured output\",\n",
    "                \"quality_score\": 5\n",
    "            }\n",
    "        \n",
    "        current_task.result = result_data\n",
    "        current_task.status = \"completed\"\n",
    "        current_task.execution_time = time.time() - start_time\n",
    "        \n",
    "        # Move to completed tasks\n",
    "        state.completed_tasks.append(current_task)\n",
    "        state.map_results.append(result_data)\n",
    "        \n",
    "        print(f\"✅ Chunk analyzed in {current_task.execution_time:.2f}s (Tokens: {total_tokens})\")\n",
    "    \n",
    "    # Continue with next task\n",
    "    return Command(goto=\"map_coordinator\", update=state.model_dump())\n",
    "\n",
    "def reduce_aggregator(state: MapReduceState) -> Command:\n",
    "    \"\"\"Aggregates results from parallel processing\"\"\"\n",
    "    print(\"🔄 Reduce Aggregator: Combining parallel results\")\n",
    "    \n",
    "    if state.map_results:\n",
    "        # Aggregate all map results\n",
    "        aggregation_prompt = f\"\"\"\n",
    "        You are aggregating results from parallel data processing.\n",
    "        \n",
    "        Map Results Summary:\n",
    "        - Total chunks processed: {len(state.map_results)}\n",
    "        - Average quality score: {sum(r.get('quality_score', 5) for r in state.map_results) / len(state.map_results):.2f}\n",
    "        \n",
    "        Individual Results:\n",
    "        {json.dumps(state.map_results[:5], indent=2)[:2000]}  # Show first 5 results\n",
    "        \n",
    "        Provide a comprehensive aggregated analysis with:\n",
    "        1. Overall patterns across all chunks\n",
    "        2. Combined metrics and statistics\n",
    "        3. Key insights and recommendations\n",
    "        4. Quality assessment\n",
    "        5. Performance summary\n",
    "        \n",
    "        Format as JSON with appropriate structure.\n",
    "        \"\"\"\n",
    "        \n",
    "        input_tokens = count_tokens(aggregation_prompt, \"gpt-4\")\n",
    "        response = manager_llm.invoke([HumanMessage(content=aggregation_prompt)])\n",
    "        output_tokens = count_tokens(response.content, \"gpt-4\")\n",
    "        \n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        state.total_tokens_used += total_tokens\n",
    "        state.estimated_cost += estimate_cost(total_tokens, \"gpt-4\")\n",
    "        \n",
    "        # Parse and store final result\n",
    "        try:\n",
    "            state.reduce_result = json.loads(response.content)\n",
    "        except json.JSONDecodeError:\n",
    "            state.reduce_result = {\n",
    "                \"summary\": response.content,\n",
    "                \"total_chunks\": len(state.map_results),\n",
    "                \"aggregation_method\": \"text_summary\"\n",
    "            }\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        total_execution_time = sum(t.execution_time or 0 for t in state.completed_tasks)\n",
    "        avg_execution_time = total_execution_time / len(state.completed_tasks) if state.completed_tasks else 0\n",
    "        \n",
    "        state.performance_metrics = {\n",
    "            \"total_tasks\": len(state.completed_tasks),\n",
    "            \"total_execution_time\": total_execution_time,\n",
    "            \"average_task_time\": avg_execution_time,\n",
    "            \"total_tokens\": state.total_tokens_used,\n",
    "            \"estimated_cost\": state.estimated_cost,\n",
    "            \"cost_per_task\": state.estimated_cost / len(state.completed_tasks) if state.completed_tasks else 0\n",
    "        }\n",
    "        \n",
    "        state.messages.append(response)\n",
    "        \n",
    "        print(f\"🎯 Aggregation complete: {len(state.map_results)} results combined\")\n",
    "        print(f\"📊 Performance: {total_execution_time:.2f}s total, ${state.estimated_cost:.4f} cost\")\n",
    "    \n",
    "    return Command(finish=state.model_dump())\n",
    "\n",
    "print(\"⚡ Map-reduce pattern implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance and Cost Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationConfig(BaseModel):\n",
    "    \"\"\"Configuration for performance optimization\"\"\"\n",
    "    max_tokens_per_request: int = 1000\n",
    "    use_gpt4_for_complex_only: bool = True\n",
    "    enable_caching: bool = True\n",
    "    enable_batching: bool = True\n",
    "    max_batch_size: int = 5\n",
    "    cost_threshold: float = 1.0  # Stop if cost exceeds this\n",
    "    token_threshold: int = 10000  # Switch to cheaper model\n",
    "\n",
    "class OptimizedState(BaseModel):\n",
    "    \"\"\"State with optimization tracking\"\"\"\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    tasks: List[Task] = Field(default_factory=list)\n",
    "    completed_tasks: List[Task] = Field(default_factory=list)\n",
    "    optimization_config: OptimizationConfig = Field(default_factory=OptimizationConfig)\n",
    "    cost_tracking: Dict[str, float] = Field(default_factory=dict)\n",
    "    token_tracking: Dict[str, int] = Field(default_factory=dict)\n",
    "    model_usage: Dict[str, int] = Field(default_factory=dict)\n",
    "    cache_hits: int = 0\n",
    "    cache_misses: int = 0\n",
    "    optimization_decisions: List[str] = Field(default_factory=list)\n",
    "    performance_metrics: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "class OptimizedAgent:\n",
    "    \"\"\"Agent with performance and cost optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, config: OptimizationConfig):\n",
    "        self.config = config\n",
    "        self.response_cache = {}\n",
    "        self.model_selector = ModelSelector()\n",
    "    \n",
    "    def select_optimal_model(self, task_complexity: str, current_cost: float) -> ChatOpenAI:\n",
    "        \"\"\"Select the most cost-effective model for the task\"\"\"\n",
    "        \n",
    "        # Cost-based selection\n",
    "        if current_cost > self.config.cost_threshold:\n",
    "            return fast_llm  # Cheapest option\n",
    "        \n",
    "        # Complexity-based selection\n",
    "        if task_complexity in [\"simple\", \"routine\"]:\n",
    "            return worker_llm  # GPT-3.5-turbo\n",
    "        elif task_complexity in [\"moderate\", \"analysis\"]:\n",
    "            return manager_llm if self.config.use_gpt4_for_complex_only else worker_llm\n",
    "        else:  # complex, strategic\n",
    "            return executive_llm  # GPT-4\n",
    "    \n",
    "    def optimize_prompt(self, prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Optimize prompt for token efficiency\"\"\"\n",
    "        \n",
    "        # If prompt is too long, summarize it\n",
    "        current_tokens = count_tokens(prompt)\n",
    "        \n",
    "        if current_tokens > max_tokens:\n",
    "            # Use fast model to summarize the prompt\n",
    "            summary_prompt = f\"\"\"\n",
    "            Summarize this prompt to under {max_tokens} tokens while preserving key instructions:\n",
    "            \n",
    "            {prompt[:2000]}...\n",
    "            \n",
    "            Keep all essential details and instructions.\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = fast_llm.invoke([HumanMessage(content=summary_prompt)])\n",
    "                optimized_prompt = response.content\n",
    "                \n",
    "                # Verify it's actually shorter\n",
    "                if count_tokens(optimized_prompt) < current_tokens:\n",
    "                    return optimized_prompt\n",
    "            except Exception:\n",
    "                pass  # Fall back to truncation\n",
    "            \n",
    "            # Fallback: simple truncation\n",
    "            return prompt[:max_tokens * 4]  # Rough token-to-char ratio\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def get_cached_response(self, prompt_hash: str) -> Optional[str]:\n",
    "        \"\"\"Get cached response if available\"\"\"\n",
    "        if self.config.enable_caching:\n",
    "            return self.response_cache.get(prompt_hash)\n",
    "        return None\n",
    "    \n",
    "    def cache_response(self, prompt_hash: str, response: str):\n",
    "        \"\"\"Cache response for future use\"\"\"\n",
    "        if self.config.enable_caching:\n",
    "            self.response_cache[prompt_hash] = response\n",
    "\n",
    "class ModelSelector:\n",
    "    \"\"\"Intelligent model selection based on task requirements\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.complexity_keywords = {\n",
    "            \"simple\": [\"summarize\", \"list\", \"extract\", \"format\", \"convert\"],\n",
    "            \"moderate\": [\"analyze\", \"compare\", \"evaluate\", \"research\", \"process\"],\n",
    "            \"complex\": [\"strategic\", \"design\", \"architect\", \"plan\", \"solve\", \"create\"]\n",
    "        }\n",
    "    \n",
    "    def assess_complexity(self, task_description: str) -> str:\n",
    "        \"\"\"Assess task complexity based on description\"\"\"\n",
    "        task_lower = task_description.lower()\n",
    "        \n",
    "        complexity_scores = {\"simple\": 0, \"moderate\": 0, \"complex\": 0}\n",
    "        \n",
    "        for complexity, keywords in self.complexity_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in task_lower:\n",
    "                    complexity_scores[complexity] += 1\n",
    "        \n",
    "        # Return the complexity with highest score\n",
    "        return max(complexity_scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "def optimized_processor(state: OptimizedState) -> Command:\n",
    "    \"\"\"Optimized processor with cost and performance tracking\"\"\"\n",
    "    print(\"⚡ Optimized Processor: Processing with performance optimization\")\n",
    "    \n",
    "    agent = OptimizedAgent(state.optimization_config)\n",
    "    current_cost = sum(state.cost_tracking.values())\n",
    "    \n",
    "    # Check cost threshold\n",
    "    if current_cost > state.optimization_config.cost_threshold:\n",
    "        state.optimization_decisions.append(f\"Stopped processing: cost threshold ${state.optimization_config.cost_threshold} exceeded\")\n",
    "        return Command(finish=state.model_dump())\n",
    "    \n",
    "    pending_tasks = [t for t in state.tasks if t.status == \"pending\"]\n",
    "    \n",
    "    if pending_tasks:\n",
    "        # Process tasks in batches for efficiency\n",
    "        batch_size = min(state.optimization_config.max_batch_size, len(pending_tasks))\n",
    "        batch_tasks = pending_tasks[:batch_size]\n",
    "        \n",
    "        for task in batch_tasks:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Assess task complexity\n",
    "            complexity = agent.model_selector.assess_complexity(task.description)\n",
    "            \n",
    "            # Select optimal model\n",
    "            selected_model = agent.select_optimal_model(complexity, current_cost)\n",
    "            model_name = \"gpt-4\" if selected_model == executive_llm or selected_model == manager_llm else \"gpt-3.5-turbo\"\n",
    "            \n",
    "            # Track model usage\n",
    "            state.model_usage[model_name] = state.model_usage.get(model_name, 0) + 1\n",
    "            \n",
    "            # Create optimized prompt\n",
    "            base_prompt = f\"\"\"\n",
    "            Task: {task.title}\n",
    "            Description: {task.description}\n",
    "            Complexity: {complexity}\n",
    "            \n",
    "            Provide efficient, focused output for this {complexity} task.\n",
    "            \"\"\"\n",
    "            \n",
    "            optimized_prompt = agent.optimize_prompt(\n",
    "                base_prompt, \n",
    "                state.optimization_config.max_tokens_per_request\n",
    "            )\n",
    "            \n",
    "            # Check cache first\n",
    "            import hashlib\n",
    "            prompt_hash = hashlib.md5(optimized_prompt.encode()).hexdigest()\n",
    "            cached_response = agent.get_cached_response(prompt_hash)\n",
    "            \n",
    "            if cached_response:\n",
    "                response_content = cached_response\n",
    "                state.cache_hits += 1\n",
    "                tokens_used = count_tokens(cached_response, model_name)\n",
    "                state.optimization_decisions.append(f\"Used cached response for task {task.task_id}\")\n",
    "            else:\n",
    "                # Make API call\n",
    "                state.cache_misses += 1\n",
    "                \n",
    "                input_tokens = count_tokens(optimized_prompt, model_name)\n",
    "                response = selected_model.invoke([HumanMessage(content=optimized_prompt)])\n",
    "                output_tokens = count_tokens(response.content, model_name)\n",
    "                tokens_used = input_tokens + output_tokens\n",
    "                \n",
    "                response_content = response.content\n",
    "                \n",
    "                # Cache the response\n",
    "                agent.cache_response(prompt_hash, response_content)\n",
    "            \n",
    "            # Update tracking\n",
    "            cost_for_task = estimate_cost(tokens_used, model_name)\n",
    "            state.cost_tracking[task.task_id] = cost_for_task\n",
    "            state.token_tracking[task.task_id] = tokens_used\n",
    "            \n",
    "            # Complete task\n",
    "            task.result = response_content\n",
    "            task.status = \"completed\"\n",
    "            task.completed_at = datetime.now().isoformat()\n",
    "            task.actual_tokens = tokens_used\n",
    "            \n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            state.optimization_decisions.append(\n",
    "                f\"Task {task.task_id}: {model_name}, {tokens_used} tokens, ${cost_for_task:.4f}, {execution_time:.2f}s\"\n",
    "            )\n",
    "            \n",
    "            # Move to completed\n",
    "            state.completed_tasks.append(task)\n",
    "        \n",
    "        # Remove processed tasks\n",
    "        state.tasks = [t for t in state.tasks if t.task_id not in [bt.task_id for bt in batch_tasks]]\n",
    "        \n",
    "        print(f\"✅ Processed batch of {len(batch_tasks)} tasks\")\n",
    "    \n",
    "    # Calculate final performance metrics\n",
    "    total_cost = sum(state.cost_tracking.values())\n",
    "    total_tokens = sum(state.token_tracking.values())\n",
    "    cache_hit_rate = state.cache_hits / (state.cache_hits + state.cache_misses) if (state.cache_hits + state.cache_misses) > 0 else 0\n",
    "    \n",
    "    state.performance_metrics = {\n",
    "        \"total_cost\": total_cost,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"completed_tasks\": len(state.completed_tasks),\n",
    "        \"cache_hit_rate\": cache_hit_rate,\n",
    "        \"model_usage\": dict(state.model_usage),\n",
    "        \"avg_cost_per_task\": total_cost / len(state.completed_tasks) if state.completed_tasks else 0,\n",
    "        \"avg_tokens_per_task\": total_tokens / len(state.completed_tasks) if state.completed_tasks else 0\n",
    "    }\n",
    "    \n",
    "    if state.tasks:  # More tasks to process\n",
    "        return Command(goto=\"optimized_processor\", update=state.model_dump())\n",
    "    else:\n",
    "        return Command(finish=state.model_dump())\n",
    "\n",
    "print(\"🎯 Performance optimization components ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Building and Testing Complete Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hierarchical system\n",
    "def create_hierarchical_system():\n",
    "    \"\"\"Create complete hierarchical multi-agent system\"\"\"\n",
    "    \n",
    "    graph = StateGraph(HierarchicalState)\n",
    "    \n",
    "    # Add all agent levels\n",
    "    graph.add_node(\"executive\", executive_agent)\n",
    "    graph.add_node(\"manager\", manager_agent)\n",
    "    graph.add_node(\"worker_coordinator\", worker_coordinator)\n",
    "    graph.add_node(\"research_worker\", research_worker)\n",
    "    graph.add_node(\"analysis_worker\", analysis_worker)\n",
    "    \n",
    "    # Add a general worker for other tasks\n",
    "    def general_worker(state: HierarchicalState) -> Command:\n",
    "        if state.current_task:\n",
    "            task = state.current_task\n",
    "            response = worker_llm.invoke([HumanMessage(content=f\"Complete this task: {task.description}\")])\n",
    "            task.result = response.content\n",
    "            task.status = \"completed\"\n",
    "            task.completed_at = datetime.now().isoformat()\n",
    "            state.completed_tasks.append(task)\n",
    "            state.tasks = [t for t in state.tasks if t.task_id != task.task_id]\n",
    "            state.current_task = None\n",
    "        return Command(goto=\"worker_coordinator\", update=state.model_dump())\n",
    "    \n",
    "    graph.add_node(\"general_worker\", general_worker)\n",
    "    \n",
    "    # Results aggregator\n",
    "    def results_aggregator(state: HierarchicalState) -> Command:\n",
    "        print(\"📊 Results Aggregator: Compiling final results\")\n",
    "        \n",
    "        if state.completed_tasks:\n",
    "            results_summary = \"\\n\\n\".join([\n",
    "                f\"Task: {t.title}\\nResult: {t.result[:200]}...\"\n",
    "                for t in state.completed_tasks\n",
    "            ])\n",
    "            \n",
    "            final_prompt = f\"\"\"\n",
    "            Compile a comprehensive executive summary from these completed tasks:\n",
    "            \n",
    "            Strategic Plan:\n",
    "            {state.strategic_plan.get('plan', 'N/A')[:300]}...\n",
    "            \n",
    "            Operational Plan:\n",
    "            {state.operational_plan.get('plan', 'N/A')[:300]}...\n",
    "            \n",
    "            Completed Tasks:\n",
    "            {results_summary[:1500]}...\n",
    "            \n",
    "            Provide executive summary with key achievements, insights, and recommendations.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = executive_llm.invoke([HumanMessage(content=final_prompt)])\n",
    "            state.messages.append(response)\n",
    "            \n",
    "            print(f\"✅ Executive summary generated\")\n",
    "        \n",
    "        return Command(finish=state.model_dump())\n",
    "    \n",
    "    graph.add_node(\"results_aggregator\", results_aggregator)\n",
    "    \n",
    "    # Connect the flow\n",
    "    graph.add_edge(START, \"executive\")\n",
    "    \n",
    "    # Compile with persistence\n",
    "    saver = SqliteSaver.from_conn_string(\"hierarchical_system.db\")\n",
    "    return graph.compile(checkpointer=saver)\n",
    "\n",
    "# Build map-reduce system\n",
    "def create_mapreduce_system():\n",
    "    \"\"\"Create map-reduce parallel processing system\"\"\"\n",
    "    \n",
    "    graph = StateGraph(MapReduceState)\n",
    "    \n",
    "    graph.add_node(\"map_coordinator\", map_coordinator)\n",
    "    graph.add_node(\"map_analyzer\", map_analyzer)\n",
    "    graph.add_node(\"reduce_aggregator\", reduce_aggregator)\n",
    "    \n",
    "    # Add summarizer for different operations\n",
    "    def map_summarizer(state: MapReduceState) -> Command:\n",
    "        current_task = next((t for t in state.parallel_tasks if t.status == \"processing\"), None)\n",
    "        if current_task:\n",
    "            data_content = json.dumps(current_task.data_chunk, indent=2)\n",
    "            response = fast_llm.invoke([HumanMessage(content=f\"Summarize this data: {data_content[:500]}\")])\n",
    "            current_task.result = {\"summary\": response.content}\n",
    "            current_task.status = \"completed\"\n",
    "            state.completed_tasks.append(current_task)\n",
    "            state.map_results.append(current_task.result)\n",
    "        return Command(goto=\"map_coordinator\", update=state.model_dump())\n",
    "    \n",
    "    def map_processor(state: MapReduceState) -> Command:\n",
    "        current_task = next((t for t in state.parallel_tasks if t.status == \"processing\"), None)\n",
    "        if current_task:\n",
    "            data_content = json.dumps(current_task.data_chunk, indent=2)\n",
    "            response = worker_llm.invoke([HumanMessage(content=f\"Process this data: {data_content[:500]}\")])\n",
    "            current_task.result = {\"processed_output\": response.content}\n",
    "            current_task.status = \"completed\"\n",
    "            state.completed_tasks.append(current_task)\n",
    "            state.map_results.append(current_task.result)\n",
    "        return Command(goto=\"map_coordinator\", update=state.model_dump())\n",
    "    \n",
    "    graph.add_node(\"map_summarizer\", map_summarizer)\n",
    "    graph.add_node(\"map_processor\", map_processor)\n",
    "    \n",
    "    graph.add_edge(START, \"map_coordinator\")\n",
    "    \n",
    "    saver = SqliteSaver.from_conn_string(\"mapreduce_system.db\")\n",
    "    return graph.compile(checkpointer=saver)\n",
    "\n",
    "# Build optimized system\n",
    "def create_optimized_system():\n",
    "    \"\"\"Create cost-optimized processing system\"\"\"\n",
    "    \n",
    "    graph = StateGraph(OptimizedState)\n",
    "    graph.add_node(\"optimized_processor\", optimized_processor)\n",
    "    graph.add_edge(START, \"optimized_processor\")\n",
    "    \n",
    "    saver = SqliteSaver.from_conn_string(\"optimized_system.db\")\n",
    "    return graph.compile(checkpointer=saver)\n",
    "\n",
    "print(\"🏗️ All advanced systems created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Testing the Complete Advanced System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Hierarchical System\n",
    "print(\"🧪 Testing Hierarchical Multi-Agent System...\")\n",
    "\n",
    "hierarchical_app = create_hierarchical_system()\n",
    "\n",
    "# Create test tasks for hierarchical system\n",
    "test_tasks = [\n",
    "    Task(\n",
    "        task_id=\"task_001\",\n",
    "        title=\"Market Research on AI Tools\",\n",
    "        description=\"Research the current market for AI development tools and frameworks\",\n",
    "        task_type=TaskType.RESEARCH,\n",
    "        priority=1,\n",
    "        assigned_level=AgentLevel.WORKER\n",
    "    ),\n",
    "    Task(\n",
    "        task_id=\"task_002\",\n",
    "        title=\"Competitive Analysis\",\n",
    "        description=\"Analyze competitors in the AI agent development space\",\n",
    "        task_type=TaskType.ANALYSIS,\n",
    "        priority=2,\n",
    "        assigned_level=AgentLevel.WORKER\n",
    "    )\n",
    "]\n",
    "\n",
    "hierarchical_state = HierarchicalState(tasks=test_tasks)\n",
    "hierarchical_config = {\"configurable\": {\"thread_id\": \"hierarchical-test\"}}\n",
    "\n",
    "print(\"\\n🚀 Running hierarchical system...\")\n",
    "hierarchical_result = hierarchical_app.invoke(hierarchical_state, config=hierarchical_config)\n",
    "\n",
    "print(f\"\\n📊 Hierarchical System Results:\")\n",
    "print(f\"✅ Completed Tasks: {len(hierarchical_result.completed_tasks)}\")\n",
    "print(f\"💰 Total Cost: ${hierarchical_result.estimated_cost:.4f}\")\n",
    "print(f\"🔢 Total Tokens: {hierarchical_result.total_tokens_used}\")\n",
    "print(f\"📋 Strategic Plan: {'✅ Created' if hierarchical_result.strategic_plan else '❌ Missing'}\")\n",
    "print(f\"📋 Operational Plan: {'✅ Created' if hierarchical_result.operational_plan else '❌ Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Map-Reduce System\n",
    "print(\"\\n🧪 Testing Map-Reduce Parallel Processing...\")\n",
    "\n",
    "mapreduce_app = create_mapreduce_system()\n",
    "\n",
    "# Create test data for parallel processing\n",
    "test_data = [\n",
    "    {\"id\": 1, \"category\": \"technology\", \"value\": 100, \"description\": \"AI development tools\"},\n",
    "    {\"id\": 2, \"category\": \"technology\", \"value\": 150, \"description\": \"Machine learning frameworks\"},\n",
    "    {\"id\": 3, \"category\": \"business\", \"value\": 200, \"description\": \"Market analysis data\"},\n",
    "    {\"id\": 4, \"category\": \"business\", \"value\": 120, \"description\": \"Customer feedback\"},\n",
    "    {\"id\": 5, \"category\": \"technology\", \"value\": 180, \"description\": \"Performance metrics\"}\n",
    "]\n",
    "\n",
    "mapreduce_state = MapReduceState(\n",
    "    input_data=test_data,\n",
    "    operation_type=\"analysis\"\n",
    ")\n",
    "mapreduce_config = {\"configurable\": {\"thread_id\": \"mapreduce-test\"}}\n",
    "\n",
    "print(\"\\n🚀 Running map-reduce system...\")\n",
    "mapreduce_result = mapreduce_app.invoke(mapreduce_state, config=mapreduce_config)\n",
    "\n",
    "print(f\"\\n📊 Map-Reduce Results:\")\n",
    "print(f\"⚡ Parallel Tasks: {len(mapreduce_result.completed_tasks)}\")\n",
    "print(f\"📈 Map Results: {len(mapreduce_result.map_results)}\")\n",
    "print(f\"🎯 Reduce Result: {'✅ Generated' if mapreduce_result.reduce_result else '❌ Missing'}\")\n",
    "print(f\"💰 Total Cost: ${mapreduce_result.estimated_cost:.4f}\")\n",
    "print(f\"⏱️ Performance Metrics:\")\n",
    "for key, value in mapreduce_result.performance_metrics.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Optimized System\n",
    "print(\"\\n🧪 Testing Cost-Optimized Processing...\")\n",
    "\n",
    "optimized_app = create_optimized_system()\n",
    "\n",
    "# Create tasks with different complexity levels\n",
    "optimization_tasks = [\n",
    "    Task(\n",
    "        task_id=\"opt_001\",\n",
    "        title=\"Simple Data Extraction\",\n",
    "        description=\"Extract key metrics from a simple dataset\",\n",
    "        task_type=TaskType.EXECUTION,\n",
    "        priority=3,\n",
    "        assigned_level=AgentLevel.WORKER\n",
    "    ),\n",
    "    Task(\n",
    "        task_id=\"opt_002\",\n",
    "        title=\"Strategic Planning\",\n",
    "        description=\"Develop a comprehensive strategic plan for AI product development\",\n",
    "        task_type=TaskType.STRATEGIC,\n",
    "        priority=1,\n",
    "        assigned_level=AgentLevel.EXECUTIVE\n",
    "    ),\n",
    "    Task(\n",
    "        task_id=\"opt_003\",\n",
    "        title=\"Data Analysis\",\n",
    "        description=\"Analyze customer behavior patterns and trends\",\n",
    "        task_type=TaskType.ANALYSIS,\n",
    "        priority=2,\n",
    "        assigned_level=AgentLevel.WORKER\n",
    "    )\n",
    "]\n",
    "\n",
    "optimization_config = OptimizationConfig(\n",
    "    max_tokens_per_request=500,\n",
    "    use_gpt4_for_complex_only=True,\n",
    "    enable_caching=True,\n",
    "    cost_threshold=0.50\n",
    ")\n",
    "\n",
    "optimized_state = OptimizedState(\n",
    "    tasks=optimization_tasks,\n",
    "    optimization_config=optimization_config\n",
    ")\n",
    "optimized_config = {\"configurable\": {\"thread_id\": \"optimized-test\"}}\n",
    "\n",
    "print(\"\\n🚀 Running optimized system...\")\n",
    "optimized_result = optimized_app.invoke(optimized_state, config=optimized_config)\n",
    "\n",
    "print(f\"\\n📊 Optimization Results:\")\n",
    "print(f\"✅ Completed Tasks: {len(optimized_result.completed_tasks)}\")\n",
    "print(f\"💰 Total Cost: ${optimized_result.performance_metrics.get('total_cost', 0):.4f}\")\n",
    "print(f\"🔢 Total Tokens: {optimized_result.performance_metrics.get('total_tokens', 0)}\")\n",
    "print(f\"🎯 Cache Hit Rate: {optimized_result.performance_metrics.get('cache_hit_rate', 0):.2%}\")\n",
    "print(f\"📱 Model Usage: {optimized_result.performance_metrics.get('model_usage', {})}\")\n",
    "print(f\"💡 Optimization Decisions:\")\n",
    "for decision in optimized_result.optimization_decisions[-3:]:  # Show last 3 decisions\n",
    "    print(f\"   {decision}\")\n",
    "\n",
    "print(\"\\n🎉 All advanced systems tested successfully!\")\n",
    "print(f\"\\n📈 Total Systems Performance Summary:\")\n",
    "print(f\"🏗️ Hierarchical: ${hierarchical_result.estimated_cost:.4f} cost, {hierarchical_result.total_tokens_used} tokens\")\n",
    "print(f\"⚡ Map-Reduce: ${mapreduce_result.estimated_cost:.4f} cost, {mapreduce_result.total_tokens_used} tokens\")\n",
    "print(f\"🎯 Optimized: ${optimized_result.performance_metrics.get('total_cost', 0):.4f} cost, {optimized_result.performance_metrics.get('total_tokens', 0)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🛠️ Practical Exercises (30 minutes)\n",
    "\n",
    "### Exercise 1: Build a Content Production Pipeline\n",
    "**Goal**: Create a hierarchical system for automated content production.\n",
    "\n",
    "**Requirements**:\n",
    "- Content strategist (executive level) for content planning\n",
    "- Content manager (manager level) for workflow coordination\n",
    "- Research writers, editors, and publishers (worker level)\n",
    "- Integrate Tavily for research and fact-checking\n",
    "- Implement cost optimization for different content types\n",
    "- Include quality control and approval workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your implementation here\n",
    "class ContentPiece(BaseModel):\n",
    "    \"\"\"Content piece for production pipeline\"\"\"\n",
    "    # TODO: Define your content structure\n",
    "    pass\n",
    "\n",
    "class ContentProductionState(BaseModel):\n",
    "    \"\"\"State for content production pipeline\"\"\"\n",
    "    # TODO: Define your content production state\n",
    "    pass\n",
    "\n",
    "def content_strategist_agent(state: ContentProductionState) -> Command:\n",
    "    \"\"\"Develops content strategy and editorial calendar\"\"\"\n",
    "    # TODO: Implement content strategy logic\n",
    "    pass\n",
    "\n",
    "def research_writer_agent(state: ContentProductionState) -> Command:\n",
    "    \"\"\"Researches and writes content using tools\"\"\"\n",
    "    # TODO: Implement research and writing logic with Tavily integration\n",
    "    pass\n",
    "\n",
    "# TODO: Create and test your content production pipeline\n",
    "print(\"📝 Exercise 1: Implement your content production pipeline here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a Real-time Data Processing System\n",
    "**Goal**: Build a map-reduce system for processing large datasets in real-time.\n",
    "\n",
    "**Requirements**:\n",
    "- Data ingestion and preprocessing workers\n",
    "- Parallel analysis workers for different data types\n",
    "- Real-time aggregation and reporting\n",
    "- API integration for external data sources\n",
    "- Performance monitoring and auto-scaling\n",
    "- Error handling and data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your implementation here\n",
    "class DataStream(BaseModel):\n",
    "    \"\"\"Data stream for real-time processing\"\"\"\n",
    "    # TODO: Define your data stream structure\n",
    "    pass\n",
    "\n",
    "class RealTimeProcessingState(BaseModel):\n",
    "    \"\"\"State for real-time data processing\"\"\"\n",
    "    # TODO: Define your real-time processing state\n",
    "    pass\n",
    "\n",
    "def data_ingestion_worker(state: RealTimeProcessingState) -> Command:\n",
    "    \"\"\"Ingests data from various sources\"\"\"\n",
    "    # TODO: Implement data ingestion logic\n",
    "    pass\n",
    "\n",
    "def parallel_analyzer_worker(state: RealTimeProcessingState) -> Command:\n",
    "    \"\"\"Analyzes data chunks in parallel\"\"\"\n",
    "    # TODO: Implement parallel analysis logic\n",
    "    pass\n",
    "\n",
    "# TODO: Create and test your real-time processing system\n",
    "print(\"📊 Exercise 2: Implement your real-time data processing system here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Create an AI-Powered Business Intelligence Platform\n",
    "**Goal**: Build a comprehensive BI platform with advanced AI capabilities.\n",
    "\n",
    "**Advanced Requirements**:\n",
    "- Executive dashboard with strategic insights\n",
    "- Automated report generation and distribution\n",
    "- Predictive analytics and forecasting\n",
    "- Natural language query interface\n",
    "- Multi-source data integration (APIs, databases, web scraping)\n",
    "- Cost-optimized processing for different report types\n",
    "- Real-time alerts and anomaly detection\n",
    "- Role-based access and personalized insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Your implementation here\n",
    "class BusinessInsight(BaseModel):\n",
    "    \"\"\"Business insight structure\"\"\"\n",
    "    # TODO: Design comprehensive business insight schema\n",
    "    pass\n",
    "\n",
    "class BIplatformState(BaseModel):\n",
    "    \"\"\"State for business intelligence platform\"\"\"\n",
    "    # TODO: Design complex BI platform state\n",
    "    pass\n",
    "\n",
    "# TODO: Implement your AI-powered business intelligence platform\n",
    "print(\"🎯 Challenge: Build your AI-powered business intelligence platform here\")\n",
    "print(\"💡 Hint: Consider data sources, analysis types, user roles, and cost optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 📚 Solutions and Best Practices\n",
    "\n",
    "### Exercise 1 Solution: Content Production Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete solution for Exercise 1\n",
    "from enum import Enum\n",
    "\n",
    "class ContentType(str, Enum):\n",
    "    BLOG_POST = \"blog_post\"\n",
    "    ARTICLE = \"article\"\n",
    "    SOCIAL_MEDIA = \"social_media\"\n",
    "    NEWSLETTER = \"newsletter\"\n",
    "    WHITE_PAPER = \"white_paper\"\n",
    "\n",
    "class ContentStatus(str, Enum):\n",
    "    PLANNED = \"planned\"\n",
    "    RESEARCHING = \"researching\"\n",
    "    WRITING = \"writing\"\n",
    "    EDITING = \"editing\"\n",
    "    REVIEW = \"review\"\n",
    "    APPROVED = \"approved\"\n",
    "    PUBLISHED = \"published\"\n",
    "\n",
    "class ContentPiece(BaseModel):\n",
    "    content_id: str\n",
    "    title: str\n",
    "    content_type: ContentType\n",
    "    target_audience: str\n",
    "    keywords: List[str] = Field(default_factory=list)\n",
    "    research_sources: List[str] = Field(default_factory=list)\n",
    "    content_brief: str\n",
    "    draft_content: Optional[str] = None\n",
    "    final_content: Optional[str] = None\n",
    "    status: ContentStatus = ContentStatus.PLANNED\n",
    "    assigned_writer: Optional[str] = None\n",
    "    quality_score: Optional[float] = None\n",
    "    word_count: int = 0\n",
    "    estimated_tokens: int = 0\n",
    "    actual_tokens: int = 0\n",
    "    created_at: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "class ContentProductionState(BaseModel):\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    content_calendar: List[ContentPiece] = Field(default_factory=list)\n",
    "    current_content: Optional[ContentPiece] = None\n",
    "    editorial_guidelines: Dict[str, Any] = Field(default_factory=dict)\n",
    "    quality_standards: Dict[str, Any] = Field(default_factory=dict)\n",
    "    production_metrics: Dict[str, Any] = Field(default_factory=dict)\n",
    "    total_tokens_used: int = 0\n",
    "    estimated_cost: float = 0.0\n",
    "\n",
    "def content_strategist_agent(state: ContentProductionState) -> Command:\n",
    "    \"\"\"Develops content strategy and editorial calendar\"\"\"\n",
    "    print(\"📋 Content Strategist: Planning content strategy\")\n",
    "    \n",
    "    # Create content strategy\n",
    "    strategy_prompt = f\"\"\"\n",
    "    You are a content strategist. Plan a content calendar for the next month.\n",
    "    \n",
    "    Current content queue: {len(state.content_calendar)} pieces\n",
    "    \n",
    "    Create a strategic plan including:\n",
    "    1. Content themes and topics\n",
    "    2. Content type distribution\n",
    "    3. Target audience segments\n",
    "    4. SEO keyword strategy\n",
    "    5. Publishing schedule\n",
    "    \n",
    "    Provide actionable content briefs for upcoming pieces.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = executive_llm.invoke([HumanMessage(content=strategy_prompt)])\n",
    "    \n",
    "    # Store editorial guidelines\n",
    "    state.editorial_guidelines = {\n",
    "        \"strategy\": response.content,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"agent\": \"content_strategist\"\n",
    "    }\n",
    "    \n",
    "    state.messages.append(response)\n",
    "    \n",
    "    return Command(goto=\"content_manager\", update=state.model_dump())\n",
    "\n",
    "def research_writer_agent(state: ContentProductionState) -> Command:\n",
    "    \"\"\"Researches and writes content using tools\"\"\"\n",
    "    print(\"✍️ Research Writer: Creating content with research\")\n",
    "    \n",
    "    if state.current_content:\n",
    "        content = state.current_content\n",
    "        \n",
    "        # Research phase using Tavily\n",
    "        research_results = []\n",
    "        for keyword in content.keywords[:3]:  # Limit research\n",
    "            result = tool_manager.search_web(f\"{keyword} {content.target_audience}\", max_results=2)\n",
    "            if result.success:\n",
    "                research_results.extend(result.result)\n",
    "        \n",
    "        # Writing phase\n",
    "        research_summary = \"\\n\".join([\n",
    "            f\"- {r['title']}: {r['content'][:100]}...\"\n",
    "            for r in research_results[:5]\n",
    "        ])\n",
    "        \n",
    "        writing_prompt = f\"\"\"\n",
    "        Write a {content.content_type.value} with the following specifications:\n",
    "        \n",
    "        Title: {content.title}\n",
    "        Brief: {content.content_brief}\n",
    "        Target Audience: {content.target_audience}\n",
    "        Keywords to include: {', '.join(content.keywords)}\n",
    "        \n",
    "        Research Sources:\n",
    "        {research_summary}\n",
    "        \n",
    "        Create engaging, well-structured content that meets editorial standards.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Select model based on content type complexity\n",
    "        model = manager_llm if content.content_type in [ContentType.WHITE_PAPER, ContentType.ARTICLE] else worker_llm\n",
    "        model_name = \"gpt-4\" if model == manager_llm else \"gpt-3.5-turbo\"\n",
    "        \n",
    "        response = model.invoke([HumanMessage(content=writing_prompt)])\n",
    "        \n",
    "        tokens_used = count_tokens(writing_prompt + response.content, model_name)\n",
    "        state.total_tokens_used += tokens_used\n",
    "        state.estimated_cost += estimate_cost(tokens_used, model_name)\n",
    "        \n",
    "        # Update content\n",
    "        content.draft_content = response.content\n",
    "        content.word_count = len(response.content.split())\n",
    "        content.actual_tokens = tokens_used\n",
    "        content.status = ContentStatus.EDITING\n",
    "        content.assigned_writer = \"research_writer\"\n",
    "        \n",
    "        print(f\"✅ Content drafted: {content.word_count} words, {tokens_used} tokens\")\n",
    "    \n",
    "    return Command(goto=\"content_editor\", update=state.model_dump())\n",
    "\n",
    "def content_editor_agent(state: ContentProductionState) -> Command:\n",
    "    \"\"\"Edits and improves content quality\"\"\"\n",
    "    print(\"📝 Content Editor: Reviewing and editing content\")\n",
    "    \n",
    "    if state.current_content and state.current_content.draft_content:\n",
    "        content = state.current_content\n",
    "        \n",
    "        editing_prompt = f\"\"\"\n",
    "        You are a professional editor. Review and improve this {content.content_type.value}:\n",
    "        \n",
    "        Title: {content.title}\n",
    "        Target Audience: {content.target_audience}\n",
    "        \n",
    "        Draft Content:\n",
    "        {content.draft_content[:2000]}...\n",
    "        \n",
    "        Provide:\n",
    "        1. Edited version with improvements\n",
    "        2. Quality score (1-10)\n",
    "        3. Key improvements made\n",
    "        4. Recommendations for further enhancement\n",
    "        \n",
    "        Focus on clarity, engagement, and target audience fit.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = manager_llm.invoke([HumanMessage(content=editing_prompt)])\n",
    "        \n",
    "        # Extract quality score (simplified)\n",
    "        try:\n",
    "            import re\n",
    "            score_match = re.search(r'quality score[:\\s]*(\\d+)', response.content.lower())\n",
    "            content.quality_score = float(score_match.group(1)) if score_match else 7.0\n",
    "        except:\n",
    "            content.quality_score = 7.0\n",
    "        \n",
    "        content.final_content = response.content\n",
    "        content.status = ContentStatus.REVIEW\n",
    "        \n",
    "        print(f\"✅ Content edited: Quality score {content.quality_score}/10\")\n",
    "    \n",
    "    return Command(goto=\"content_manager\", update=state.model_dump())\n",
    "\n",
    "def content_manager_agent(state: ContentProductionState) -> Command:\n",
    "    \"\"\"Manages content workflow and quality control\"\"\"\n",
    "    print(\"👔 Content Manager: Managing production workflow\")\n",
    "    \n",
    "    # Check if there's content in review\n",
    "    if state.current_content and state.current_content.status == ContentStatus.REVIEW:\n",
    "        content = state.current_content\n",
    "        \n",
    "        # Approve if quality score is sufficient\n",
    "        if content.quality_score and content.quality_score >= 7.0:\n",
    "            content.status = ContentStatus.APPROVED\n",
    "            print(f\"✅ Content approved: {content.title}\")\n",
    "            \n",
    "            # Move to completed calendar\n",
    "            state.content_calendar.append(content)\n",
    "            state.current_content = None\n",
    "        else:\n",
    "            print(f\"⚠️ Content needs improvement: Quality score {content.quality_score}\")\n",
    "            content.status = ContentStatus.WRITING  # Send back for revision\n",
    "    \n",
    "    # Start new content if none in progress\n",
    "    elif not state.current_content:\n",
    "        # Create new content piece\n",
    "        new_content = ContentPiece(\n",
    "            content_id=\"content_001\",\n",
    "            title=\"The Future of AI in Business Operations\",\n",
    "            content_type=ContentType.BLOG_POST,\n",
    "            target_audience=\"business executives\",\n",
    "            keywords=[\"artificial intelligence\", \"business automation\", \"digital transformation\"],\n",
    "            content_brief=\"Explore how AI is transforming business operations and what executives need to know\"\n",
    "        )\n",
    "        \n",
    "        state.current_content = new_content\n",
    "        new_content.status = ContentStatus.RESEARCHING\n",
    "        \n",
    "        return Command(goto=\"research_writer\", update=state.model_dump())\n",
    "    \n",
    "    # Calculate production metrics\n",
    "    approved_content = [c for c in state.content_calendar if c.status == ContentStatus.APPROVED]\n",
    "    \n",
    "    state.production_metrics = {\n",
    "        \"total_pieces\": len(state.content_calendar),\n",
    "        \"approved_pieces\": len(approved_content),\n",
    "        \"average_quality\": sum(c.quality_score for c in approved_content if c.quality_score) / len(approved_content) if approved_content else 0,\n",
    "        \"total_words\": sum(c.word_count for c in approved_content),\n",
    "        \"total_cost\": state.estimated_cost,\n",
    "        \"avg_cost_per_piece\": state.estimated_cost / len(approved_content) if approved_content else 0\n",
    "    }\n",
    "    \n",
    "    return Command(finish=state.model_dump())\n",
    "\n",
    "# Create content production system\n",
    "def create_content_production_system():\n",
    "    graph = StateGraph(ContentProductionState)\n",
    "    \n",
    "    graph.add_node(\"content_strategist\", content_strategist_agent)\n",
    "    graph.add_node(\"content_manager\", content_manager_agent)\n",
    "    graph.add_node(\"research_writer\", research_writer_agent)\n",
    "    graph.add_node(\"content_editor\", content_editor_agent)\n",
    "    \n",
    "    graph.add_edge(START, \"content_strategist\")\n",
    "    \n",
    "    saver = SqliteSaver.from_conn_string(\"content_production.db\")\n",
    "    return graph.compile(checkpointer=saver)\n",
    "\n",
    "# Test content production system\n",
    "content_app = create_content_production_system()\n",
    "content_state = ContentProductionState()\n",
    "content_config = {\"configurable\": {\"thread_id\": \"content-production-test\"}}\n",
    "\n",
    "print(\"\\n📝 Testing content production pipeline...\")\n",
    "content_result = content_app.invoke(content_state, config=content_config)\n",
    "\n",
    "print(f\"\\n📊 Content Production Results:\")\n",
    "print(f\"📋 Production Metrics:\")\n",
    "for key, value in content_result.production_metrics.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "if content_result.content_calendar:\n",
    "    approved_content = [c for c in content_result.content_calendar if c.status == ContentStatus.APPROVED]\n",
    "    for content in approved_content:\n",
    "        print(f\"\\n✅ Approved Content: {content.title}\")\n",
    "        print(f\"   Type: {content.content_type.value}, Words: {content.word_count}\")\n",
    "        print(f\"   Quality: {content.quality_score}/10, Tokens: {content.actual_tokens}\")\n",
    "\n",
    "print(\"\\n✅ Content production pipeline solution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🔧 Troubleshooting Common Issues\n",
    "\n",
    "### Tool Integration Issues\n",
    "```python\n",
    "# ❌ Common issue: Tool failures not handled\n",
    "result = tavily_client.search(query)  # May fail\n",
    "\n",
    "# ✅ Proper error handling\n",
    "try:\n",
    "    result = tool_manager.search_web(query)\n",
    "    if not result.success:\n",
    "        # Fallback to knowledge-based approach\n",
    "        return fallback_processing(query)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Tool error: {e}\")\n",
    "    return error_response\n",
    "```\n",
    "\n",
    "### Performance Optimization Issues\n",
    "```python\n",
    "# ✅ Monitor token usage\n",
    "def track_tokens(prompt: str, response: str, model: str):\n",
    "    input_tokens = count_tokens(prompt, model)\n",
    "    output_tokens = count_tokens(response, model)\n",
    "    total_cost = estimate_cost(input_tokens + output_tokens, model)\n",
    "    \n",
    "    # Log for monitoring\n",
    "    logger.info(f\"Tokens: {input_tokens + output_tokens}, Cost: ${total_cost:.4f}\")\n",
    "```\n",
    "\n",
    "### Parallel Processing Issues\n",
    "```python\n",
    "# ✅ Handle task failures in parallel processing\n",
    "def safe_parallel_task(task_data):\n",
    "    try:\n",
    "        return process_task(task_data)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"task_id\": task_data.get(\"id\")}\n",
    "```\n",
    "\n",
    "### Cost Control Issues\n",
    "```python\n",
    "# ✅ Implement cost controls\n",
    "class CostController:\n",
    "    def __init__(self, max_cost: float):\n",
    "        self.max_cost = max_cost\n",
    "        self.current_cost = 0.0\n",
    "    \n",
    "    def can_proceed(self, estimated_cost: float) -> bool:\n",
    "        return (self.current_cost + estimated_cost) <= self.max_cost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 📖 Summary and Course Completion\n",
    "\n",
    "### What You've Mastered Across 5 Days:\n",
    "✅ **Day 1**: LangGraph foundations, nodes, edges, and basic workflows  \n",
    "✅ **Day 2**: State management, persistence, and error recovery  \n",
    "✅ **Day 3**: Memory systems, knowledge management, and long-term retention  \n",
    "✅ **Day 4**: Multi-agent communication, handoffs, and secure messaging  \n",
    "✅ **Day 5**: Advanced architectures, tool integration, and optimization  \n",
    "\n",
    "### Advanced Patterns You've Learned:\n",
    "- **Hierarchical Systems**: Executive, manager, and worker agent layers\n",
    "- **Parallel Processing**: Map-reduce patterns for scalable computation\n",
    "- **Tool Integration**: Tavily search, API calls, and external services\n",
    "- **Performance Optimization**: Model selection, caching, and cost control\n",
    "- **Production Deployment**: Monitoring, error handling, and scalability\n",
    "\n",
    "### Key Technical Skills:\n",
    "- Multi-agent system architecture and design\n",
    "- OpenAI API optimization and cost management\n",
    "- Tool integration and error handling\n",
    "- State management and persistence strategies\n",
    "- Performance monitoring and optimization\n",
    "- Production deployment best practices\n",
    "\n",
    "### Production-Ready Capabilities:\n",
    "- Build scalable multi-agent systems\n",
    "- Implement cost-effective AI workflows\n",
    "- Integrate external tools and APIs\n",
    "- Handle errors and edge cases gracefully\n",
    "- Monitor performance and optimize costs\n",
    "- Deploy and maintain LangGraph applications\n",
    "\n",
    "### Next Steps for Continued Learning:\n",
    "1. **Explore LangGraph Cloud**: Deploy your systems to production\n",
    "2. **Advanced Tool Integration**: Build custom tools and integrations\n",
    "3. **Multi-Modal Agents**: Integrate vision and audio capabilities\n",
    "4. **Enterprise Patterns**: Implement enterprise-grade security and compliance\n",
    "5. **Community Contribution**: Share your patterns and learn from others\n",
    "\n",
    "### Resources for Continued Growth:\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangChain Academy](https://academy.langchain.com/)\n",
    "- [OpenAI API Best Practices](https://platform.openai.com/docs/guides/production-best-practices)\n",
    "- [LangGraph Community](https://github.com/langchain-ai/langgraph)\n",
    "\n",
    "**🎉 Congratulations! You've completed the comprehensive 5-day LangGraph course and are now ready to build production-grade multi-agent AI systems!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course completion summary\n",
    "print(\"🧹 Course complete! Final database files created:\")\n",
    "import os\n",
    "db_files = [f for f in os.listdir('.') if f.endswith('.db')]\n",
    "for db_file in db_files:\n",
    "    if any(x in db_file for x in ['hierarchical', 'mapreduce', 'optimized', 'content']):\n",
    "        print(f\"  📁 {db_file}\")\n",
    "\n",
    "print(\"\\n🎉 5-Day LangGraph Course Complete!\")\n",
    "print(\"🚀 You've mastered:\")\n",
    "print(\"   📚 Day 1: LangGraph Foundations\")\n",
    "print(\"   💾 Day 2: State Management & Persistence\")\n",
    "print(\"   🧠 Day 3: Memory Systems & Knowledge Management\")\n",
    "print(\"   🤝 Day 4: Multi-Agent Communication & Handoffs\")\n",
    "print(\"   🏗️ Day 5: Advanced Architectures & Tool Integration\")\n",
    "print(\"\\n🎯 You're now ready to build production-grade multi-agent AI systems with LangGraph!\")\n",
    "print(\"🌟 Keep building amazing AI applications!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.0"
   }
 }, "nbformat": 4, "nbformat_minor": 4}